{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81bdd5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMN QUALITY ANALYSIS\n",
      "==================================================\n",
      "\n",
      "ACCOUNT_NAME:\n",
      "------------------------------\n",
      "account_name ← BEST\n",
      "  Overall: 0.95\n",
      "  Complete: 0.82\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "AccountName\n",
      "  Overall: 0.91\n",
      "  Complete: 0.70\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "Account Name\n",
      "  Overall: 0.88\n",
      "  Complete: 0.60\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "\n",
      "CONTACT_EMAIL:\n",
      "------------------------------\n",
      "contact_email ← BEST\n",
      "  Overall: 0.80\n",
      "  Complete: 0.76\n",
      "  Quality: 1.00\n",
      "  Format: 0.35\n",
      "  DataType: 1.00\n",
      "\n",
      "Contact Email\n",
      "  Overall: 0.58\n",
      "  Complete: 0.75\n",
      "  Quality: 0.31\n",
      "  Format: 0.31\n",
      "  DataType: 1.00\n",
      "\n",
      "\n",
      "CREATED_DATE:\n",
      "------------------------------\n",
      "created_date ← BEST\n",
      "  Overall: 0.82\n",
      "  Complete: 0.76\n",
      "  Quality: 0.65\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "Created Date\n",
      "  Overall: 0.77\n",
      "  Complete: 0.78\n",
      "  Quality: 0.67\n",
      "  Format: 0.67\n",
      "  DataType: 1.00\n",
      "\n",
      "\n",
      "LEAD_SOURCE:\n",
      "------------------------------\n",
      "lead_source ← BEST\n",
      "  Overall: 0.94\n",
      "  Complete: 0.80\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "Lead Source\n",
      "  Overall: 0.87\n",
      "  Complete: 0.57\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "\n",
      "OPPORTUNITY_AMOUNT:\n",
      "------------------------------\n",
      "Opportunity Amount ← BEST\n",
      "  Overall: 0.83\n",
      "  Complete: 0.66\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 0.65\n",
      "\n",
      "opportunity_amount\n",
      "  Overall: 0.81\n",
      "  Complete: 0.59\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 0.66\n",
      "\n",
      "\n",
      "IS_ACTIVE:\n",
      "------------------------------\n",
      "is_active ← BEST\n",
      "  Overall: 0.95\n",
      "  Complete: 0.82\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "Is Active\n",
      "  Overall: 0.93\n",
      "  Complete: 0.87\n",
      "  Quality: 1.00\n",
      "  Format: 0.83\n",
      "  DataType: 1.00\n",
      "\n",
      "\n",
      "SFDC_ID:\n",
      "------------------------------\n",
      "SFDC ID ← BEST\n",
      "  Overall: 0.89\n",
      "  Complete: 0.63\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "sfdc_id\n",
      "  Overall: 0.87\n",
      "  Complete: 0.81\n",
      "  Quality: 0.75\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "\n",
      "ANNUAL_REVENUE:\n",
      "------------------------------\n",
      "annual_revenue ← BEST\n",
      "  Overall: 0.92\n",
      "  Complete: 0.74\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "Annual Revenue\n",
      "  Overall: 0.83\n",
      "  Complete: 0.85\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 0.36\n",
      "\n",
      "\n",
      "LOCATION:\n",
      "------------------------------\n",
      "State ← BEST\n",
      "  Overall: 0.96\n",
      "  Complete: 0.87\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "City\n",
      "  Overall: 0.91\n",
      "  Complete: 0.70\n",
      "  Quality: 1.00\n",
      "  Format: 1.00\n",
      "  DataType: 1.00\n",
      "\n",
      "\n",
      "RANDOM NOTES ANALYSIS:\n",
      "------------------------------\n",
      "Total records: 500\n",
      "'See notes': 104 occurrences (20.8%)\n",
      "'Valid': 91 occurrences (18.2%)\n",
      "'N/A': 0 occurrences (0.0%)\n",
      "Empty/null: 305 occurrences (61.0%)\n",
      "\n",
      "\n",
      "DEAL SCORE RELATIONSHIP ANALYSIS:\n",
      "==================================================\n",
      "Total records analyzed: 340\n",
      "\n",
      "Correlations:\n",
      "  deal_score_vs_opportunity: 0.01\n",
      "  deal_score_vs_revenue: 0.041\n",
      "  opportunity_vs_revenue: -0.105\n",
      "\n",
      "Strongest relationship: Deal Score vs annual_revenue\n",
      "Deal Score is predictive: False\n",
      "\n",
      "Deal Score Range Analysis:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "class ColumnQualityAnalyzer:\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "    \n",
    "    def completeness_score(self, column: str) -> float:\n",
    "        # How much of the column is filled\n",
    "        if column not in self.df.columns:\n",
    "            return 0.0\n",
    "        return self.df[column].notna().mean()\n",
    "    \n",
    "    def quality_score(self, column: str) -> float:\n",
    "        # How much of the data looks valid\n",
    "        if column not in self.df.columns:\n",
    "            return 0.0\n",
    "        \n",
    "        series = self.df[column].dropna()\n",
    "        if len(series) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        errors = ['not_a_date', 'noemail', 'invalid@', 'BAD_ID', 'N/A', 'NaT', 'ERROR']\n",
    "        error_count = sum(1 for value in series.astype(str) \n",
    "                         if any(error in str(value) for error in errors))\n",
    "        \n",
    "        return 1.0 - (error_count / len(series))\n",
    "    \n",
    "    def format_consistency_score(self, column: str) -> float:\n",
    "        # Check format consistency\n",
    "        if column not in self.df.columns:\n",
    "            return 0.0\n",
    "            \n",
    "        series = self.df[column].dropna()\n",
    "        if len(series) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Email format \n",
    "        if 'email' in column.lower():\n",
    "            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "            valid_emails = sum(1 for value in series.astype(str) \n",
    "                             if re.match(email_pattern, str(value)))\n",
    "            return valid_emails / len(series)\n",
    "        \n",
    "        # Date format\n",
    "        elif 'date' in column.lower() or 'created' in column.lower():\n",
    "            valid_dates = 0\n",
    "            for value in series:\n",
    "                try:\n",
    "                    # Try common date formats\n",
    "                    pd.to_datetime(str(value))\n",
    "                    valid_dates += 1\n",
    "                except:\n",
    "                    pass\n",
    "            return valid_dates / len(series)\n",
    "        \n",
    "        # Boolean format\n",
    "        elif 'active' in column.lower() or 'is_' in column.lower():\n",
    "            valid_bools = sum(1 for value in series.astype(str).str.lower() \n",
    "                            if str(value).lower() in ['true', 'false', '1', '0', 'yes', 'no'])\n",
    "            return valid_bools / len(series)\n",
    "        \n",
    "        return 1.0\n",
    "    \n",
    "    def data_type_consistency_score(self, column: str) -> float:\n",
    "        if column not in self.df.columns:\n",
    "            return 0.0\n",
    "            \n",
    "        series = self.df[column].dropna()\n",
    "        if len(series) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Amount/Revenue columns should be numeric\n",
    "        if any(word in column.lower() for word in ['amount', 'revenue', 'score']):\n",
    "            numeric_count = 0\n",
    "            for value in series:\n",
    "                try:\n",
    "                    if isinstance(value, str):\n",
    "                        if any(word in str(value).lower() for word in ['thousand', 'million', 'n/a']):\n",
    "                            continue\n",
    "                    float(value)\n",
    "                    numeric_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "            return numeric_count / len(series)\n",
    "        \n",
    "        # ID columns should be strings/text\n",
    "        elif 'id' in column.lower():\n",
    "            # Most values should be convertible to string and non-empty\n",
    "            valid_ids = sum(1 for value in series \n",
    "                          if str(value).strip() and str(value) != 'nan')\n",
    "            return valid_ids / len(series)\n",
    "        \n",
    "        return 1.0\n",
    "    \n",
    "    def overall_score(self, column: str) -> float:\n",
    "        completeness = self.completeness_score(column)\n",
    "        quality = self.quality_score(column)\n",
    "        format_consistency = self.format_consistency_score(column)\n",
    "        data_type_consistency = self.data_type_consistency_score(column)\n",
    "        \n",
    "        return (completeness * 0.3) + (quality * 0.3) + (format_consistency * 0.2) + (data_type_consistency * 0.2)\n",
    "    \n",
    "    def compare_columns(self, columns: List[str]) -> Dict:\n",
    "        results = {}\n",
    "        \n",
    "        for col in columns:\n",
    "            if col in self.df.columns:\n",
    "                results[col] = {\n",
    "                    'completeness': self.completeness_score(col),\n",
    "                    'quality': self.quality_score(col),\n",
    "                    'format_consistency': self.format_consistency_score(col),\n",
    "                    'data_type_consistency': self.data_type_consistency_score(col),\n",
    "                    'overall': self.overall_score(col)\n",
    "                }\n",
    "        \n",
    "        sorted_results = dict(sorted(results.items(), key=lambda x: x[1]['overall'], reverse=True))\n",
    "        best_column = next(iter(sorted_results)) if sorted_results else None\n",
    "        \n",
    "        return {\n",
    "            'scores': sorted_results,\n",
    "            'recommendation': best_column\n",
    "        }\n",
    "\n",
    "    def analyze_random_notes(self) -> Dict:\n",
    "        column = 'Random Notes'\n",
    "        \n",
    "        if column not in self.df.columns:\n",
    "            return {\n",
    "                'error': f\"Column '{column}' not found in dataset\"\n",
    "            }\n",
    "        \n",
    "        total_rows = len(self.df)\n",
    "        series = self.df[column]\n",
    "        \n",
    "        # Count each specific value\n",
    "        see_notes_count = sum(1 for value in series if str(value).strip() == 'See notes')\n",
    "        valid_count = sum(1 for value in series if str(value).strip() == 'Valid')\n",
    "        na_count = sum(1 for value in series if str(value).strip() == 'N/A')\n",
    "        empty_null_count = sum(1 for value in series if pd.isna(value) or str(value).strip() == '' or str(value) == 'nan')\n",
    "        \n",
    "        # Calculate percentages\n",
    "        see_notes_pct = (see_notes_count / total_rows) * 100\n",
    "        valid_pct = (valid_count / total_rows) * 100\n",
    "        na_pct = (na_count / total_rows) * 100\n",
    "        empty_null_pct = (empty_null_count / total_rows) * 100\n",
    "        \n",
    "        return {\n",
    "            'total_rows': total_rows,\n",
    "            'see_notes': {'count': see_notes_count, 'percentage': see_notes_pct},\n",
    "            'valid': {'count': valid_count, 'percentage': valid_pct},\n",
    "            'na': {'count': na_count, 'percentage': na_pct},\n",
    "            'empty_null': {'count': empty_null_count, 'percentage': empty_null_pct},\n",
    "        }\n",
    "    \n",
    "    def analyze_duplicates(self, duplicate_groups: Dict[str, List[str]]):\n",
    "        print(\"COLUMN QUALITY ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for group_name, columns in duplicate_groups.items():\n",
    "\n",
    "            existing_cols = [col for col in columns if col in self.df.columns]\n",
    "            \n",
    "            if len(existing_cols) <= 1:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n{group_name.upper()}:\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            comparison = self.compare_columns(existing_cols)\n",
    "            recommended = comparison['recommendation']\n",
    "            \n",
    "            for col, scores in comparison['scores'].items():\n",
    "                status = \" ← BEST\" if col == recommended else \"\"\n",
    "                print(f\"{col}{status}\")\n",
    "                print(f\"  Overall: {scores['overall']:.2f}\")\n",
    "                print(f\"  Complete: {scores['completeness']:.2f}\")\n",
    "                print(f\"  Quality: {scores['quality']:.2f}\")\n",
    "                print(f\"  Format: {scores['format_consistency']:.2f}\")\n",
    "                print(f\"  DataType: {scores['data_type_consistency']:.2f}\")\n",
    "                print()\n",
    "        \n",
    "        print(\"\\nRANDOM NOTES ANALYSIS:\")\n",
    "        print(\"-\" * 30)\n",
    "        notes_analysis = self.analyze_random_notes()\n",
    "        \n",
    "        if 'error' in notes_analysis:\n",
    "            print(notes_analysis['error'])\n",
    "        else:\n",
    "            print(f\"Total records: {notes_analysis['total_rows']}\")\n",
    "            print(f\"'See notes': {notes_analysis['see_notes']['count']} occurrences ({notes_analysis['see_notes']['percentage']:.1f}%)\")\n",
    "            print(f\"'Valid': {notes_analysis['valid']['count']} occurrences ({notes_analysis['valid']['percentage']:.1f}%)\")\n",
    "            print(f\"'N/A': {notes_analysis['na']['count']} occurrences ({notes_analysis['na']['percentage']:.1f}%)\")\n",
    "            print(f\"Empty/null: {notes_analysis['empty_null']['count']} occurrences ({notes_analysis['empty_null']['percentage']:.1f}%)\")\n",
    "    \n",
    "    def analyze_deal_score_relationships(self) -> Dict:\n",
    "        deal_score_col = 'Deal Score'\n",
    "        opp_amount_col = 'opportunity_amount'\n",
    "        annual_rev_col = 'annual_revenue'\n",
    "        \n",
    "        missing_cols = []\n",
    "        for col in [deal_score_col, opp_amount_col, annual_rev_col]:\n",
    "            if col not in self.df.columns:\n",
    "                missing_cols.append(col)\n",
    "        \n",
    "        if missing_cols:\n",
    "            return {'error': f\"Missing columns: {missing_cols}\"}\n",
    "        \n",
    "        df_clean = self.df[[deal_score_col, opp_amount_col, annual_rev_col]].copy()\n",
    "        \n",
    "        df_clean = df_clean.dropna()\n",
    "        \n",
    "        if len(df_clean) == 0:\n",
    "            return {'error': 'No complete records found for analysis'}\n",
    "        \n",
    "        deal_opp_corr = df_clean[deal_score_col].corr(df_clean[opp_amount_col])\n",
    "        deal_revenue_corr = df_clean[deal_score_col].corr(df_clean[annual_rev_col])\n",
    "        opp_revenue_corr = df_clean[opp_amount_col].corr(df_clean[annual_rev_col])\n",
    "        \n",
    "        df_clean['score_range'] = pd.cut(df_clean[deal_score_col], \n",
    "                                    bins=[0, 0.3, 0.7, 1.0], \n",
    "                                    labels=['Low (0-0.3)', 'Medium (0.3-0.7)', 'High (0.7-1.0)'])\n",
    "        \n",
    "        range_stats = {}\n",
    "        for score_range in df_clean['score_range'].cat.categories:\n",
    "            range_data = df_clean[df_clean['score_range'] == score_range]\n",
    "            if len(range_data) > 0:\n",
    "                range_stats[score_range] = {\n",
    "                    'count': len(range_data),\n",
    "                    'avg_opportunity': range_data[opp_amount_col].mean(),\n",
    "                    'avg_revenue': range_data[annual_rev_col].mean()\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'total_records': len(df_clean),\n",
    "            'correlations': {\n",
    "                'deal_score_vs_opportunity': round(deal_opp_corr, 3),\n",
    "                'deal_score_vs_revenue': round(deal_revenue_corr, 3),\n",
    "                'opportunity_vs_revenue': round(opp_revenue_corr, 3)\n",
    "            },\n",
    "            'score_range_analysis': range_stats,\n",
    "            'interpretation': {\n",
    "                'strongest_relationship': 'opportunity_amount' if abs(deal_opp_corr) > abs(deal_revenue_corr) else 'annual_revenue',\n",
    "                'deal_score_predictive': abs(deal_opp_corr) > 0.3 or abs(deal_revenue_corr) > 0.3\n",
    "            }\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    \n",
    "    duplicate_groups = {\n",
    "        'account_name': ['Account Name', 'account_name', 'AccountName'],\n",
    "        'contact_email': ['Contact Email', 'contact_email'],\n",
    "        'created_date': ['Created Date', 'created_date'],\n",
    "        'lead_source': ['Lead Source', 'lead_source'],\n",
    "        'opportunity_amount': ['Opportunity Amount', 'opportunity_amount'],\n",
    "        'is_active': ['Is Active', 'is_active'],\n",
    "        'sfdc_id': ['SFDC ID', 'sfdc_id'],\n",
    "        'annual_revenue': ['Annual Revenue', 'annual_revenue'],\n",
    "        'location': ['City', 'State']\n",
    "    }\n",
    "    \n",
    "    df = pd.read_csv('data/DirtySalesforceData.csv')\n",
    "    analyzer = ColumnQualityAnalyzer(df)\n",
    "    analyzer.analyze_duplicates(duplicate_groups)\n",
    "\n",
    "   \n",
    "\n",
    "    df2 = pd.read_csv('IntermediateSalesforceData.csv')  # Use clean data\n",
    "    analyzer = ColumnQualityAnalyzer(df2)\n",
    "    \n",
    "    # Run deal score analysis\n",
    "    print(\"\\n\\nDEAL SCORE RELATIONSHIP ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "    deal_results = analyzer.analyze_deal_score_relationships()\n",
    "    \n",
    "    if 'error' in deal_results:\n",
    "        print(f\"Error: {deal_results['error']}\")\n",
    "    else:\n",
    "        print(f\"Total records analyzed: {deal_results['total_records']}\")\n",
    "        print(\"\\nCorrelations:\")\n",
    "        for relationship, correlation in deal_results['correlations'].items():\n",
    "            print(f\"  {relationship}: {correlation}\")\n",
    "        \n",
    "        print(f\"\\nStrongest relationship: Deal Score vs {deal_results['interpretation']['strongest_relationship']}\")\n",
    "        print(f\"Deal Score is predictive: {deal_results['interpretation']['deal_score_predictive']}\")\n",
    "        \n",
    "        print(\"\\nDeal Score Range Analysis:\")\n",
    "        for score_range, stats in deal_results['score_range_analysis'].items():\n",
    "            print(f\"  {score_range}: {stats['count']} records\")\n",
    "            print(f\"    Avg Opportunity: ${stats['avg_opportunity']:,.2f}\")\n",
    "            print(f\"    Avg Revenue: ${stats['avg_revenue']:,.2f}\")\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
