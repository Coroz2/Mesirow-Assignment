{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleansing:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        self.duplicate_groups = {\n",
    "            'account_name': ['Account Name', 'account_name', 'AccountName'],\n",
    "            'contact_email': ['Contact Email', 'contact_email'],\n",
    "            'created_date': ['Created Date', 'created_date'],\n",
    "            'lead_source': ['Lead Source', 'lead_source'],\n",
    "            'opportunity_amount': ['Opportunity Amount', 'opportunity_amount'],\n",
    "            'is_active': ['Is Active', 'is_active'],\n",
    "            'sfdc_id': ['SFDC ID', 'sfdc_id'],\n",
    "            'annual_revenue': ['Annual Revenue', 'annual_revenue']\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def is_valid_email(self, email: str) -> bool:\n",
    "        # Check if email is one of the valid emails\n",
    "        if pd.isna(email) or email is None:\n",
    "            return False\n",
    "        return email in ['help@globex.com', 'contact@acme.com']\n",
    "    \n",
    "    def is_placeholder_email(self, email: str) -> bool:\n",
    "        # Check if email is a placeholder \n",
    "        if pd.isna(email) or email is None or email == '':\n",
    "            return True\n",
    "        placeholders = ['noemail', 'invalid@', 'user@', 'missing.com', 'placeholder']\n",
    "        return any(placeholder in str(email).lower() for placeholder in placeholders)\n",
    "    \n",
    "    def get_corresponding_account(self, email: str) -> Optional[str]:\n",
    "        # Get the corresponding account name for a valid email\n",
    "        if email == 'help@globex.com':\n",
    "            return 'Globex'\n",
    "        elif email == 'contact@acme.com':\n",
    "            return 'Acme Corp'\n",
    "        return None\n",
    "    \n",
    "    def generate_email_for_account(self, account_name: str) -> Optional[str]:\n",
    "        # Generate corresponding email for Globex or Acme Corp\n",
    "        if account_name == 'Globex':\n",
    "            return 'help@globex.com'\n",
    "        elif account_name == 'Acme Corp':\n",
    "            return 'contact@acme.com'\n",
    "        return None\n",
    "    \n",
    "    def consolidate_account_and_email(self) -> pd.DataFrame:\n",
    "\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_account_name'] = None\n",
    "        result_df['consolidated_contact_email'] = None\n",
    "        \n",
    "        account_columns = ['account_name', 'AccountName', 'Account Name']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "\n",
    "            contact_email_lower_val = row.get('contact_email')\n",
    "            contact_email_val = row.get('Contact Email')\n",
    "            \n",
    "            contact_email_lower_valid = self.is_valid_email(contact_email_lower_val)\n",
    "            contact_email_valid = self.is_valid_email(contact_email_val)\n",
    "            \n",
    "            account_values = {}\n",
    "            for col in account_columns:\n",
    "                if col in result_df.columns:\n",
    "                    account_values[col] = row.get(col)\n",
    "            \n",
    "            final_account = None\n",
    "            final_email = None\n",
    "            \n",
    "            for col in account_columns:\n",
    "                if (col in account_values and \n",
    "                    pd.notna(account_values[col]) and \n",
    "                    account_values[col] != ''):\n",
    "                    final_account = account_values[col]\n",
    "                    break\n",
    "            \n",
    "            # If no account found, derive from valid email (all accounts empty case)\n",
    "            if final_account is None:\n",
    "                # Check contact_email first (priority)\n",
    "                if contact_email_lower_valid:\n",
    "                    final_account = self.get_corresponding_account(contact_email_lower_val)\n",
    "                    final_email = contact_email_lower_val\n",
    "                # If contact_email not valid, check Contact Email\n",
    "                elif contact_email_valid:\n",
    "                    final_account = self.get_corresponding_account(contact_email_val)\n",
    "                    final_email = contact_email_val\n",
    "            \n",
    "            if final_account is not None and final_email is None:\n",
    "                email_matched = False\n",
    "                \n",
    "                if contact_email_lower_valid:\n",
    "                    expected_account = self.get_corresponding_account(contact_email_lower_val)\n",
    "                    if final_account == expected_account:\n",
    "                        final_email = contact_email_lower_val\n",
    "                        email_matched = True\n",
    "                \n",
    "                if not email_matched and contact_email_valid:\n",
    "                    expected_account = self.get_corresponding_account(contact_email_val)\n",
    "                    if final_account == expected_account:\n",
    "                        final_email = contact_email_val\n",
    "                        email_matched = True\n",
    "                \n",
    "                # If no email match but account is Globex or Acme Corp, generate email\n",
    "                if not email_matched:\n",
    "                    generated_email = self.generate_email_for_account(final_account)\n",
    "                    if generated_email:\n",
    "                        final_email = generated_email\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_account_name'] = final_account\n",
    "            result_df.at[idx, 'consolidated_contact_email'] = final_email\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def consolidate_created_date(self) -> pd.DataFrame:\n",
    "    \n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_created_date'] = None\n",
    "        \n",
    "        created_date_columns = ['created_date', 'Created Date']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_created_date = None\n",
    "            \n",
    "            for col in created_date_columns:\n",
    "                if col in result_df.columns:\n",
    "                    raw_date = row.get(col)\n",
    "                    if (pd.isna(raw_date) or raw_date == '' or \n",
    "                        str(raw_date).lower() in ['nat', 'not_a_date', 'none']):\n",
    "                        continue\n",
    "                    \n",
    "                    formatted_date = self.format_date_to_standard(raw_date)\n",
    "                    if formatted_date:\n",
    "                        final_created_date = formatted_date\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_created_date'] = final_created_date\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def format_date_to_standard(self, date_value) -> Optional[str]:\n",
    "        if pd.isna(date_value) or date_value is None or date_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            parsed_date = pd.to_datetime(date_value, errors='coerce')\n",
    "            \n",
    "            if pd.isna(parsed_date):\n",
    "                return None\n",
    "            \n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    def consolidate_lead_source(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_lead_source'] = None\n",
    "        \n",
    "        lead_source_columns = ['lead_source', 'Lead Source']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_lead_source = None\n",
    "            \n",
    "            for col in lead_source_columns:\n",
    "                if col in result_df.columns:\n",
    "                    lead_source_val = row.get(col)\n",
    "                    if (pd.notna(lead_source_val) and \n",
    "                        lead_source_val != '' and \n",
    "                        str(lead_source_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        final_lead_source = lead_source_val\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_lead_source'] = final_lead_source\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_is_active(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_is_active'] = None\n",
    "        \n",
    "        is_active_columns = ['is_active', 'Is Active']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_is_active = None\n",
    "            \n",
    "            for col in is_active_columns:\n",
    "                if col in result_df.columns:\n",
    "                    is_active_val = row.get(col)\n",
    "                    if (pd.notna(is_active_val) and \n",
    "                        is_active_val != '' and \n",
    "                        str(is_active_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_is_active = self.standardize_is_active(is_active_val)\n",
    "                        if standardized_is_active is not None:\n",
    "                            final_is_active = standardized_is_active\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_is_active'] = final_is_active\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def standardize_is_active(self, is_active_value) -> Optional[bool]:\n",
    "        if pd.isna(is_active_value) or is_active_value is None or is_active_value == '':\n",
    "            return None\n",
    "        \n",
    "        value_str = str(is_active_value).strip().lower()\n",
    "        \n",
    "        if value_str in ['nat', 'none', 'null', '']:\n",
    "            return None\n",
    "        \n",
    "        if value_str in ['true', 't', 'yes', 'y', '1', 'active', 'on']:\n",
    "            return True\n",
    "        elif value_str in ['false', 'f', 'no', 'n', '0', 'inactive', 'off']:\n",
    "            return False\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def consolidate_sfdc_id(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_sfdc_id'] = None\n",
    "        \n",
    "        sfdc_id_columns = ['SFDC ID', 'sfdc_id']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_sfdc_id = None\n",
    "            \n",
    "            for col in sfdc_id_columns:\n",
    "                if col in result_df.columns:\n",
    "                    sfdc_id_val = row.get(col)\n",
    "                    if (pd.notna(sfdc_id_val) and \n",
    "                        sfdc_id_val != '' and \n",
    "                        str(sfdc_id_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_sfdc_id = self.standardize_sfdc_id(sfdc_id_val)\n",
    "                        if standardized_sfdc_id is not None:\n",
    "                            final_sfdc_id = standardized_sfdc_id\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_sfdc_id'] = final_sfdc_id\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def standardize_sfdc_id(self, sfdc_id_value) -> Optional[str]:\n",
    "        if pd.isna(sfdc_id_value) or sfdc_id_value is None or sfdc_id_value == '':\n",
    "            return None\n",
    "        \n",
    "        value_str = str(sfdc_id_value).strip()\n",
    "        \n",
    "        if value_str.lower() in ['nat', 'none', 'null', '']:\n",
    "            return None\n",
    "        \n",
    "        # Check for placeholder values\n",
    "        placeholder_patterns = ['abc123', 'xyz-00001', '12345', 'bad_id']\n",
    "        if value_str.lower() in placeholder_patterns:\n",
    "            return None\n",
    "        \n",
    "        return value_str\n",
    "    \n",
    "    def consolidate_monetary(self, field_name: str) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        consolidated_column = f'consolidated_{field_name}'\n",
    "        result_df[consolidated_column] = None\n",
    "        \n",
    "        field_columns = self.duplicate_groups.get(field_name, [field_name])\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_value = None\n",
    "            \n",
    "            for col in field_columns:\n",
    "                if col in result_df.columns:\n",
    "                    raw_value = row.get(col)\n",
    "                    if (pd.notna(raw_value) and \n",
    "                        raw_value != '' and \n",
    "                        str(raw_value).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_value = self.standardize_monetary(raw_value)\n",
    "                        if standardized_value is not None:\n",
    "                            final_value = standardized_value\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, consolidated_column] = final_value\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def standardize_monetary(self, monetary_value) -> Optional[float]:\n",
    "        if pd.isna(monetary_value) or monetary_value is None or monetary_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            value_str = str(monetary_value).strip()\n",
    "            \n",
    "            if value_str.lower() in ['nat', 'none', 'null', '', 'not available', 'n/a']:\n",
    "                return None\n",
    "            \n",
    "            # Check if it's already a valid number\n",
    "            if isinstance(monetary_value, (int, float)) and not pd.isna(monetary_value):\n",
    "                if monetary_value < 0:\n",
    "                    return None\n",
    "                return round(float(monetary_value), 2)\n",
    "            \n",
    "            # Word version\n",
    "            if re.match(r'^[a-zA-Z\\s]+$', value_str):\n",
    "                converted_float = self.convert_text_to_number(value_str)\n",
    "                if converted_float is None:\n",
    "                    return None\n",
    "                if converted_float < 0:\n",
    "                    return None\n",
    "                return round(converted_float, 2)\n",
    "            \n",
    "            # Remove currency symbols and common formatting for numeric values\n",
    "            value_str = re.sub(r'[$£€¥₹]', '', value_str)\n",
    "            value_str = value_str.replace(',', '').replace(' ', '') \n",
    "            \n",
    "            if value_str == '':\n",
    "                return None\n",
    "            \n",
    "            multiplier = 1\n",
    "            value_str_lower = value_str.lower()\n",
    "            if value_str_lower.endswith('k'):\n",
    "                multiplier = 1000\n",
    "                value_str = value_str[:-1]\n",
    "            elif value_str_lower.endswith('m'):\n",
    "                multiplier = 1000000\n",
    "                value_str = value_str[:-1]\n",
    "            elif value_str_lower.endswith('b'):\n",
    "                multiplier = 1000000000\n",
    "                value_str = value_str[:-1]\n",
    "            \n",
    "            try:\n",
    "                final_float = float(value_str) * multiplier\n",
    "            except ValueError:\n",
    "                return None\n",
    "            \n",
    "            if final_float < 0:\n",
    "                return None\n",
    "            \n",
    "            return round(final_float, 2)\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    def convert_text_to_number(self, text_value: str) -> Optional[float]:\n",
    "        if not text_value:\n",
    "            return None\n",
    "            \n",
    "        text_value = text_value.lower().strip()\n",
    "        \n",
    "        number_words = {\n",
    "            'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,\n",
    "            'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10,\n",
    "            'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15,\n",
    "            'sixteen': 16, 'seventeen': 17, 'eighteen': 18, 'nineteen': 19, 'twenty': 20,\n",
    "            'thirty': 30, 'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70,\n",
    "            'eighty': 80, 'ninety': 90, 'hundred': 100, 'thousand': 1000, 'million': 1000000,\n",
    "            'billion': 1000000000\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            words = text_value.split()\n",
    "            total = 0\n",
    "            current = 0\n",
    "            found_valid_word = False\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip()\n",
    "                if word in number_words:\n",
    "                    found_valid_word = True\n",
    "                    value = number_words[word]\n",
    "                    if value == 100:\n",
    "                        current = current * 100 if current > 0 else 100\n",
    "                    elif value >= 1000:\n",
    "                        total += current * value\n",
    "                        current = 0\n",
    "                    else:\n",
    "                        current += value\n",
    "            \n",
    "            total += current\n",
    "            \n",
    "            if not found_valid_word or (total == 0 and 'zero' not in text_value):\n",
    "                return None\n",
    "            \n",
    "            return float(total)\n",
    "        \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def consolidate_last_activity(self) -> pd.DataFrame:\n",
    "        \n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_last_activity'] = None\n",
    "        \n",
    "        last_activity_columns = ['Last Activity']\n",
    "        \n",
    "        # Define placeholder values to filter out\n",
    "        placeholder_values = [\n",
    "            '42', 42, 'Called Client', 'called client', 'CALLED CLIENT', ''\n",
    "        ]\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_last_activity = None\n",
    "            \n",
    "            for col in last_activity_columns:\n",
    "                if col in result_df.columns:\n",
    "                    raw_activity = row.get(col)\n",
    "                    \n",
    "                    # Skip if NaN, empty, or placeholder value\n",
    "                    if (pd.isna(raw_activity) or \n",
    "                        raw_activity == '' or \n",
    "                        raw_activity in placeholder_values or\n",
    "                        str(raw_activity).strip().lower() in [str(p).lower() for p in placeholder_values]):\n",
    "                        continue\n",
    "                    \n",
    "                    # Try to format as date using existing method\n",
    "                    formatted_date = self.format_date_to_standard(raw_activity)\n",
    "                    if formatted_date:\n",
    "                        final_last_activity = formatted_date\n",
    "                        break\n",
    "                    \n",
    "            \n",
    "            result_df.at[idx, 'consolidated_last_activity'] = final_last_activity\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_custom_field(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_custom_field'] = None\n",
    "        \n",
    "        custom_field_columns = ['Custom Field']\n",
    "        \n",
    "        placeholder_values = ['N/A', '{\"type\": null}', None, '', 'null', 'nat', 'none']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_custom_field = None\n",
    "            \n",
    "            for col in custom_field_columns:\n",
    "                if col in result_df.columns:\n",
    "                    custom_field_val = row.get(col)\n",
    "                    \n",
    "                    if (pd.notna(custom_field_val) and \n",
    "                        custom_field_val != '' and \n",
    "                        custom_field_val not in placeholder_values and\n",
    "                        str(custom_field_val).lower() not in [str(p).lower() for p in placeholder_values if p is not None]):\n",
    "                        final_custom_field = custom_field_val\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_custom_field'] = final_custom_field\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def consolidate_region(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_region'] = None\n",
    "        \n",
    "        region_columns = ['Region']\n",
    "        \n",
    "        north_america_values = ['North America', 'NA', 'N.A.', 'United States', 'US']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_region = None\n",
    "            \n",
    "            for col in region_columns:\n",
    "                if col in result_df.columns:\n",
    "                    region_val = row.get(col)\n",
    "                    \n",
    "                    if (pd.notna(region_val) and \n",
    "                        region_val != '' and \n",
    "                        str(region_val).strip() in north_america_values):\n",
    "                        final_region = 'North America'\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_region'] = final_region\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_random_notes(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_random_notes'] = None\n",
    "        \n",
    "        random_notes_columns = ['Random Notes']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_notes_flag = None\n",
    "            \n",
    "            for col in random_notes_columns:\n",
    "                if col in result_df.columns:\n",
    "                    notes_val = row.get(col)\n",
    "                    \n",
    "                    # Check if value is \"See notes\" or \"Valid\"\n",
    "                    if (pd.notna(notes_val) and \n",
    "                        notes_val != '' and \n",
    "                        str(notes_val).strip() in ['See notes', 'Valid']):\n",
    "                        final_notes_flag = 'Notes_Flag'\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_random_notes'] = final_notes_flag\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "\n",
    "    def consolidate_deal_score(self) -> pd.DataFrame:\n",
    "\n",
    "        result_df = self.df.copy()\n",
    "        result_df['consolidated_deal_score'] = None\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            deal_score_val = row.get('Deal Score')\n",
    "            \n",
    "            if pd.notna(deal_score_val):\n",
    "                result_df.at[idx, 'consolidated_deal_score'] = deal_score_val / 100.0\n",
    "            else:\n",
    "                result_df.at[idx, 'consolidated_deal_score'] = None\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_engagement_level(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        result_df['consolidated_engagement_level'] = None\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            engagement_val = row.get('Engagement Level')\n",
    "            \n",
    "            if pd.notna(engagement_val):\n",
    "                result_df.at[idx, 'consolidated_engagement_level'] = round(engagement_val, 4)\n",
    "            else:\n",
    "                result_df.at[idx, 'consolidated_engagement_level'] = None\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def create_clean_dataset(self) -> pd.DataFrame:\n",
    "        consolidated_df = self.consolidate_account_and_email()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_created_date()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_lead_source()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_monetary('opportunity_amount')\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_is_active()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_sfdc_id()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_monetary('annual_revenue')\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_last_activity()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_custom_field()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_region()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "    \n",
    "        consolidated_df = self.consolidate_random_notes()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "\n",
    "        consolidated_df = self.consolidate_deal_score()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "\n",
    "        consolidated_df = self.consolidate_engagement_level()\n",
    "        \n",
    "        clean_df = consolidated_df.copy()\n",
    "        \n",
    "        clean_df['account_name'] = consolidated_df['consolidated_account_name']\n",
    "        clean_df['contact_email'] = consolidated_df['consolidated_contact_email']\n",
    "        clean_df['created_date'] = consolidated_df['consolidated_created_date']\n",
    "        clean_df['lead_source'] = consolidated_df['consolidated_lead_source']\n",
    "        clean_df['opportunity_amount'] = consolidated_df['consolidated_opportunity_amount']\n",
    "        clean_df['is_active'] = consolidated_df['consolidated_is_active']\n",
    "        clean_df['sfdc_id'] = consolidated_df['consolidated_sfdc_id']\n",
    "        clean_df['annual_revenue'] = consolidated_df['consolidated_annual_revenue']\n",
    "        clean_df['last_activity'] = consolidated_df['consolidated_last_activity']\n",
    "        clean_df['custom_field'] = consolidated_df['consolidated_custom_field']\n",
    "        clean_df['region'] = consolidated_df['consolidated_region']\n",
    "        clean_df['notes_flag'] = consolidated_df['consolidated_random_notes']\n",
    "        clean_df['deal_score'] = consolidated_df['consolidated_deal_score']\n",
    "        clean_df['engagement_level'] = consolidated_df['consolidated_engagement_level']\n",
    "        \n",
    "        columns_to_drop = [\n",
    "            'consolidated_account_name', 'consolidated_contact_email', 'consolidated_created_date',\n",
    "            'consolidated_lead_source', 'consolidated_opportunity_amount', 'consolidated_is_active', 'consolidated_engagement_level',\n",
    "            'consolidated_sfdc_id', 'consolidated_last_activity', 'consolidated_annual_revenue', 'consolidated_deal_score',\n",
    "            'consolidated_custom_field', 'consolidated_region', 'consolidated_random_notes',\n",
    "            'Account Name', 'AccountName', 'Contact Email', 'Created Date', 'Lead Source', \n",
    "            'Opportunity Amount', 'Is Active', 'SFDC ID', 'Last Activity', 'Annual Revenue', 'Deal Score', 'Engagement Level',\n",
    "            'Custom Field', 'Region', 'Random Notes', 'Unnamed: 0', 'Unnamed: 21'  \n",
    "        ]\n",
    "        \n",
    "        columns_to_drop = [col for col in columns_to_drop if col in clean_df.columns]\n",
    "        clean_df = clean_df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        return clean_df\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Info:\n",
      "Shape: (500, 30)\n",
      "Columns: ['Account Name', 'account_name', 'AccountName', 'Contact Email', 'contact_email', 'Created Date', 'created_date', 'Lead Source', 'lead_source', 'Opportunity Amount', 'opportunity_amount', 'Is Active', 'is_active', 'SFDC ID', 'sfdc_id', 'Annual Revenue', 'annual_revenue', 'Last Activity', 'Custom Field', 'Region', 'Unnamed: 0', 'Unnamed: 21', 'Random Notes', 'Deal Score', 'Engagement Level', 'Num Calls', 'Time on Page (sec)', 'City', 'State', 'Country']\n",
      "\n",
      "Clean dataset saved as 'CleanSalesforceData.csv'\n",
      "Clean dataset shape: (500, 19)\n",
      "\n",
      "Sample of consolidated data:\n",
      "   account_name     contact_email created_date     lead_source  \\\n",
      "0     Acme Corp  contact@acme.com   2020-01-01      Trade Show   \n",
      "1        Globex   help@globex.com   2022-12-01    Social Media   \n",
      "2        Globex   help@globex.com         None  Email Campaign   \n",
      "3     Acme Corp  contact@acme.com         None    Social Media   \n",
      "4      Umbrella              None   2024-03-26             Web   \n",
      "5        Globex   help@globex.com   2024-04-03            None   \n",
      "6       Soylent              None   2025-07-17        Referral   \n",
      "7       Soylent              None   2023-12-17   Phone Inquiry   \n",
      "8     Acme Corp  contact@acme.com   2022-12-01      Trade Show   \n",
      "9     Acme Corp  contact@acme.com         None  Email Campaign   \n",
      "10     Umbrella              None   2024-06-03      Trade Show   \n",
      "11        Wonka              None   2025-07-08      Trade Show   \n",
      "12       Globex   help@globex.com   2022-12-01            None   \n",
      "13      Soylent              None   2020-01-01      Trade Show   \n",
      "14      Soylent              None   2020-01-01         Partner   \n",
      "15       Globex   help@globex.com   2023-01-26    Social Media   \n",
      "16    Acme Corp  contact@acme.com   2022-12-01        Referral   \n",
      "17       Globex   help@globex.com         None             Web   \n",
      "18      Soylent              None   2020-01-01    Social Media   \n",
      "19       Globex   help@globex.com         None  Email Campaign   \n",
      "20     Umbrella              None   2020-01-01    Social Media   \n",
      "21     Umbrella              None   2024-08-12        Referral   \n",
      "22        Wonka              None   2022-12-01      Trade Show   \n",
      "23       Globex   help@globex.com   2022-12-01  Email Campaign   \n",
      "24      Soylent              None   2020-01-01  Email Campaign   \n",
      "25       Globex   help@globex.com   2024-12-27    Social Media   \n",
      "26    Acme Corp  contact@acme.com   2022-12-01    Social Media   \n",
      "27      Soylent              None   2023-05-04  Email Campaign   \n",
      "28      Initech              None         None             Web   \n",
      "29      Soylent              None         None  Email Campaign   \n",
      "\n",
      "   opportunity_amount is_active     sfdc_id annual_revenue  Num Calls  \\\n",
      "0              1000.0     False  001B000002      1000000.0        NaN   \n",
      "1                None     False        None      1000000.0       31.0   \n",
      "2             50000.0      True  001A000001      5000000.0       38.0   \n",
      "3             50000.0     False        None      1000000.0       32.0   \n",
      "4             50000.0     False  001B000002      1000000.0        NaN   \n",
      "5             50000.0      True  001A000001       999999.0       23.0   \n",
      "6             10000.0     False  001A000001           None        3.0   \n",
      "7              1000.0      True        None           None       37.0   \n",
      "8              1000.0     False  001B000002      2500000.0        NaN   \n",
      "9             50000.0     False  001C000003      5000000.0       10.0   \n",
      "10            25000.0      True  001A000001      2500000.0       49.0   \n",
      "11            50000.0      True        None      5000000.0       13.0   \n",
      "12             1000.0      True        None      1000000.0        NaN   \n",
      "13            50000.0      True  001B000002      1000000.0       26.0   \n",
      "14            50000.0     False        None      1000000.0       17.0   \n",
      "15            50000.0      True        None       100000.0       21.0   \n",
      "16            50000.0      True  001C000003      1000000.0        NaN   \n",
      "17            25000.0      True        None      1000000.0        9.0   \n",
      "18            10000.0      None  001A000001      5000000.0       41.0   \n",
      "19               None     False  001A000001      5000000.0       44.0   \n",
      "20             1000.0     False        None      1000000.0        NaN   \n",
      "21            10000.0      True        None      1000000.0       41.0   \n",
      "22            50000.0     False  001B000002      2500000.0       37.0   \n",
      "23            50000.0      True  001B000002      1000000.0        1.0   \n",
      "24               None      True        None           None        NaN   \n",
      "25                0.0      None        None      1000000.0        8.0   \n",
      "26            10000.0      True        None      1000000.0       43.0   \n",
      "27               None      True        None       999999.0       28.0   \n",
      "28            50000.0     False  001C000003      2500000.0        NaN   \n",
      "29             1000.0      True  001A000001      5000000.0       19.0   \n",
      "\n",
      "    Time on Page (sec)           City       State        Country  \\\n",
      "0                  141        Chicago        N.Y.            NaN   \n",
      "1                  278            NaN          CA            NaN   \n",
      "2                  271        Chicago         NaN  United States   \n",
      "3                  318            NaN         NaN  United States   \n",
      "4                  399  San Francisco        N.Y.  United States   \n",
      "5                  383        Chicago    Illinois             us   \n",
      "6                  155            NaN    Illinois             us   \n",
      "7                  248       New York    Illinois            USA   \n",
      "8                  419       New York          IL  United States   \n",
      "9                  108       New York          NY             us   \n",
      "10                 182  San Francisco    Illinois  United States   \n",
      "11                 194        chicago  California             us   \n",
      "12                 279            NaN    Illinois           U.S.   \n",
      "13                  37            NaN          NY            NaN   \n",
      "14                 350            nyc          NY            NaN   \n",
      "15                 105        Chicago  California             us   \n",
      "16                 331        chicago          CA            NaN   \n",
      "17                 296            nyc          CA             us   \n",
      "18                 169            NaN  California           U.S.   \n",
      "19                 353            nyc          CA           U.S.   \n",
      "20                 360       New York         NaN           U.S.   \n",
      "21                 472            NaN         NaN            USA   \n",
      "22                  73  San Francisco        N.Y.  United States   \n",
      "23                 113       New York          NY             us   \n",
      "24                  21            nyc          IL             us   \n",
      "25                  50            NaN          IL             us   \n",
      "26                 205            NaN         NaN            USA   \n",
      "27                 107       New York  California           U.S.   \n",
      "28                 203        chicago        N.Y.             us   \n",
      "29                 134        chicago        N.Y.           U.S.   \n",
      "\n",
      "   last_activity                 custom_field         region  notes_flag  \\\n",
      "0           None  {\"type\": \"A\", \"value\": 100}  North America        None   \n",
      "1           None                         None  North America        None   \n",
      "2     2024-05-05                         None  North America        None   \n",
      "3     2024-09-13  {\"type\": \"A\", \"value\": 100}  North America        None   \n",
      "4     2024-05-05                         None  North America        None   \n",
      "5           None  {\"type\": \"A\", \"value\": 100}  North America        None   \n",
      "6     2025-01-28  {\"type\": \"B\", \"value\": 200}  North America        None   \n",
      "7           None  {\"type\": \"A\", \"value\": 100}  North America  Notes_Flag   \n",
      "8     2025-01-23                         None           None        None   \n",
      "9           None                         None  North America  Notes_Flag   \n",
      "10          None                         None           None  Notes_Flag   \n",
      "11    2024-05-05                         None           None  Notes_Flag   \n",
      "12          None  {\"type\": \"B\", \"value\": 200}           None        None   \n",
      "13    2024-05-05  {\"type\": \"A\", \"value\": 100}  North America  Notes_Flag   \n",
      "14          None  {\"type\": \"B\", \"value\": 200}  North America  Notes_Flag   \n",
      "15          None                         None           None        None   \n",
      "16          None  {\"type\": \"B\", \"value\": 200}           None  Notes_Flag   \n",
      "17          None                         None  North America  Notes_Flag   \n",
      "18    2024-08-22                         None  North America  Notes_Flag   \n",
      "19          None                         None  North America        None   \n",
      "20          None  {\"type\": \"B\", \"value\": 200}  North America        None   \n",
      "21          None  {\"type\": \"A\", \"value\": 100}  North America  Notes_Flag   \n",
      "22          None                         None  North America  Notes_Flag   \n",
      "23          None                         None  North America  Notes_Flag   \n",
      "24    2024-12-18                         None  North America        None   \n",
      "25          None                         None  North America  Notes_Flag   \n",
      "26          None                         None  North America  Notes_Flag   \n",
      "27          None                         None           None        None   \n",
      "28          None  {\"type\": \"A\", \"value\": 100}  North America  Notes_Flag   \n",
      "29          None                         None           None        None   \n",
      "\n",
      "   deal_score engagement_level  \n",
      "0         0.5             None  \n",
      "1        None           0.0636  \n",
      "2        None           0.2173  \n",
      "3         1.0             None  \n",
      "4         1.0           0.5376  \n",
      "5        None           0.7661  \n",
      "6         1.0             None  \n",
      "7         0.1           0.9034  \n",
      "8         0.5           0.5414  \n",
      "9        None             None  \n",
      "10       0.75           0.0309  \n",
      "11        1.0           0.1543  \n",
      "12       0.75             None  \n",
      "13        1.0           0.7853  \n",
      "14        1.0           0.2743  \n",
      "15        0.1             None  \n",
      "16       None           0.8991  \n",
      "17        1.0           0.0606  \n",
      "18       None             None  \n",
      "19        1.0           0.2805  \n",
      "20        1.0           0.0956  \n",
      "21       0.75             None  \n",
      "22       0.75           0.7808  \n",
      "23       0.75           0.2904  \n",
      "24        0.1             None  \n",
      "25        0.5           0.8941  \n",
      "26       None            0.053  \n",
      "27       None             None  \n",
      "28        1.0           0.9407  \n",
      "29        1.0           0.9583  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/DirtySalesforceData.csv')\n",
    "\n",
    "print(\"Original Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "cleaner = Cleansing(df)\n",
    "cleaner.create_clean_dataset()\n",
    "\n",
    "clean_df = cleaner.create_clean_dataset()\n",
    "\n",
    "clean_df.to_csv('CleanSalesforceData.csv', index=False)\n",
    "print(f\"\\nClean dataset saved as 'CleanSalesforceData.csv'\")\n",
    "print(f\"Clean dataset shape: {clean_df.shape}\")\n",
    "\n",
    "print(f\"\\nSample of consolidated data:\")\n",
    "print(clean_df.head(30))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
