{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleansing:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        self.duplicate_groups = {\n",
    "            'account_name': ['Account Name', 'account_name', 'AccountName'],\n",
    "            'contact_email': ['Contact Email', 'contact_email'],\n",
    "            'created_date': ['Created Date', 'created_date'],\n",
    "            'lead_source': ['Lead Source', 'lead_source'],\n",
    "            'opportunity_amount': ['Opportunity Amount', 'opportunity_amount'],\n",
    "            'is_active': ['Is Active', 'is_active'],\n",
    "            'sfdc_id': ['SFDC ID', 'sfdc_id'],\n",
    "            'annual_revenue': ['Annual Revenue', 'annual_revenue']\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def is_valid_email(self, email: str) -> bool:\n",
    "        # Check if email is one of the valid emails\n",
    "        if pd.isna(email) or email is None:\n",
    "            return False\n",
    "        return email in ['help@globex.com', 'contact@acme.com']\n",
    "    \n",
    "    def is_placeholder_email(self, email: str) -> bool:\n",
    "        # Check if email is a placeholder \n",
    "        if pd.isna(email) or email is None or email == '':\n",
    "            return True\n",
    "        placeholders = ['noemail', 'invalid@', 'user@', 'missing.com', 'placeholder']\n",
    "        return any(placeholder in str(email).lower() for placeholder in placeholders)\n",
    "    \n",
    "    def get_corresponding_account(self, email: str) -> Optional[str]:\n",
    "        # Get the corresponding account name for a valid email\n",
    "        if email == 'help@globex.com':\n",
    "            return 'Globex'\n",
    "        elif email == 'contact@acme.com':\n",
    "            return 'Acme Corp'\n",
    "        return None\n",
    "    \n",
    "    def generate_email_for_account(self, account_name: str) -> Optional[str]:\n",
    "        # Generate corresponding email for Globex or Acme Corp\n",
    "        if account_name == 'Globex':\n",
    "            return 'help@globex.com'\n",
    "        elif account_name == 'Acme Corp':\n",
    "            return 'contact@acme.com'\n",
    "        return None\n",
    "    \n",
    "    def consolidate_account_and_email(self) -> pd.DataFrame:\n",
    "\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_account_name'] = None\n",
    "        result_df['consolidated_contact_email'] = None\n",
    "        \n",
    "        account_columns = ['account_name', 'AccountName', 'Account Name']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "\n",
    "            contact_email_lower_val = row.get('contact_email')\n",
    "            contact_email_val = row.get('Contact Email')\n",
    "            \n",
    "            contact_email_lower_valid = self.is_valid_email(contact_email_lower_val)\n",
    "            contact_email_valid = self.is_valid_email(contact_email_val)\n",
    "            \n",
    "            account_values = {}\n",
    "            for col in account_columns:\n",
    "                if col in result_df.columns:\n",
    "                    account_values[col] = row.get(col)\n",
    "            \n",
    "            final_account = None\n",
    "            final_email = None\n",
    "            \n",
    "            for col in account_columns:\n",
    "                if (col in account_values and \n",
    "                    pd.notna(account_values[col]) and \n",
    "                    account_values[col] != ''):\n",
    "                    final_account = account_values[col]\n",
    "                    break\n",
    "            \n",
    "            # If no account found, derive from valid email (all accounts empty case)\n",
    "            if final_account is None:\n",
    "                # Check contact_email first (priority)\n",
    "                if contact_email_lower_valid:\n",
    "                    final_account = self.get_corresponding_account(contact_email_lower_val)\n",
    "                    final_email = contact_email_lower_val\n",
    "                # If contact_email not valid, check Contact Email\n",
    "                elif contact_email_valid:\n",
    "                    final_account = self.get_corresponding_account(contact_email_val)\n",
    "                    final_email = contact_email_val\n",
    "            \n",
    "            if final_account is not None and final_email is None:\n",
    "                email_matched = False\n",
    "                \n",
    "                if contact_email_lower_valid:\n",
    "                    expected_account = self.get_corresponding_account(contact_email_lower_val)\n",
    "                    if final_account == expected_account:\n",
    "                        final_email = contact_email_lower_val\n",
    "                        email_matched = True\n",
    "                \n",
    "                if not email_matched and contact_email_valid:\n",
    "                    expected_account = self.get_corresponding_account(contact_email_val)\n",
    "                    if final_account == expected_account:\n",
    "                        final_email = contact_email_val\n",
    "                        email_matched = True\n",
    "                \n",
    "                # If no email match but account is Globex or Acme Corp, generate email\n",
    "                if not email_matched:\n",
    "                    generated_email = self.generate_email_for_account(final_account)\n",
    "                    if generated_email:\n",
    "                        final_email = generated_email\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_account_name'] = final_account\n",
    "            result_df.at[idx, 'consolidated_contact_email'] = final_email\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def consolidate_created_date(self) -> pd.DataFrame:\n",
    "    \n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_created_date'] = None\n",
    "        \n",
    "        created_date_columns = ['created_date', 'Created Date']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_created_date = None\n",
    "            \n",
    "            for col in created_date_columns:\n",
    "                if col in result_df.columns:\n",
    "                    raw_date = row.get(col)\n",
    "                    if (pd.isna(raw_date) or raw_date == '' or \n",
    "                        str(raw_date).lower() in ['nat', 'not_a_date', 'none']):\n",
    "                        continue\n",
    "                    \n",
    "                    formatted_date = self.format_date_to_standard(raw_date)\n",
    "                    if formatted_date:\n",
    "                        final_created_date = formatted_date\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_created_date'] = final_created_date\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def format_date_to_standard(self, date_value) -> Optional[str]:\n",
    "        if pd.isna(date_value) or date_value is None or date_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            parsed_date = pd.to_datetime(date_value, errors='coerce')\n",
    "            \n",
    "            if pd.isna(parsed_date):\n",
    "                return None\n",
    "            \n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    def consolidate_lead_source(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_lead_source'] = None\n",
    "        \n",
    "        lead_source_columns = ['lead_source', 'Lead Source']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_lead_source = None\n",
    "            \n",
    "            for col in lead_source_columns:\n",
    "                if col in result_df.columns:\n",
    "                    lead_source_val = row.get(col)\n",
    "                    if (pd.notna(lead_source_val) and \n",
    "                        lead_source_val != '' and \n",
    "                        str(lead_source_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        final_lead_source = lead_source_val\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_lead_source'] = final_lead_source\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_opportunity_amount(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_opportunity_amount'] = None\n",
    "        \n",
    "        opportunity_amount_columns = ['Opportunity Amount', 'opportunity_amount']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_opportunity_amount = None\n",
    "            \n",
    "            for col in opportunity_amount_columns:\n",
    "                if col in result_df.columns:\n",
    "                    amount_val = row.get(col)\n",
    "                    if (pd.notna(amount_val) and \n",
    "                        amount_val != '' and \n",
    "                        str(amount_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_amount = self.standardize_opportunity_amount(amount_val)\n",
    "                        if standardized_amount is not None:\n",
    "                            final_opportunity_amount = standardized_amount\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_opportunity_amount'] = final_opportunity_amount\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def standardize_opportunity_amount(self, amount_value) -> Optional[float]:\n",
    "        if pd.isna(amount_value) or amount_value is None or amount_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            amount_str = str(amount_value).strip()\n",
    "            \n",
    "            if amount_str.lower() in ['nat', 'none', 'null', '']:\n",
    "                return None\n",
    "            \n",
    "            # Word version\n",
    "            if re.match(r'^[a-zA-Z\\s]+$', amount_str):\n",
    "                amount_float = self.convert_text_to_number(amount_str)\n",
    "                if amount_float is None:\n",
    "                    return None\n",
    "                if amount_float < 0:\n",
    "                    return None\n",
    "                return round(amount_float, 2)\n",
    "            \n",
    "            # Remove currency symbols and common formatting for numeric values\n",
    "            amount_str = re.sub(r'[$£€¥₹]', '', amount_str)\n",
    "            amount_str = amount_str.replace(',', '').replace(' ', '') \n",
    "            \n",
    "            if amount_str == '':\n",
    "                return None\n",
    "            \n",
    "            multiplier = 1\n",
    "            amount_str_lower = amount_str.lower()\n",
    "            if amount_str_lower.endswith('k'):\n",
    "                multiplier = 1000\n",
    "                amount_str = amount_str[:-1]\n",
    "            elif amount_str_lower.endswith('m'):\n",
    "                multiplier = 1000000\n",
    "                amount_str = amount_str[:-1]\n",
    "            elif amount_str_lower.endswith('b'):\n",
    "                multiplier = 1000000000\n",
    "                amount_str = amount_str[:-1]\n",
    "            \n",
    "            try:\n",
    "                amount_float = float(amount_str) * multiplier\n",
    "            except ValueError:\n",
    "                return None\n",
    "            \n",
    "            if amount_float < 0:\n",
    "                return None\n",
    "            \n",
    "            return round(amount_float, 2)\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    def convert_text_to_number(self, text_value: str) -> Optional[float]:\n",
    "        if not text_value:\n",
    "            return None\n",
    "            \n",
    "        text_value = text_value.lower().strip()\n",
    "        \n",
    "        number_words = {\n",
    "            'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,\n",
    "            'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10,\n",
    "            'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15,\n",
    "            'sixteen': 16, 'seventeen': 17, 'eighteen': 18, 'nineteen': 19, 'twenty': 20,\n",
    "            'thirty': 30, 'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70,\n",
    "            'eighty': 80, 'ninety': 90, 'hundred': 100, 'thousand': 1000, 'million': 1000000,\n",
    "            'billion': 1000000000\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            words = text_value.split()\n",
    "            total = 0\n",
    "            current = 0\n",
    "            found_valid_word = False\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip()\n",
    "                if word in number_words:\n",
    "                    found_valid_word = True\n",
    "                    value = number_words[word]\n",
    "                    if value == 100:\n",
    "                        current = current * 100 if current > 0 else 100\n",
    "                    elif value >= 1000:\n",
    "                        total += current * value\n",
    "                        current = 0\n",
    "                    else:\n",
    "                        current += value\n",
    "            \n",
    "            total += current\n",
    "            \n",
    "            if not found_valid_word or (total == 0 and 'zero' not in text_value):\n",
    "                return None\n",
    "            \n",
    "            return float(total)\n",
    "        \n",
    "        except Exception:\n",
    "            return None\n",
    "    def consolidate_is_active(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_is_active'] = None\n",
    "        \n",
    "        is_active_columns = ['is_active', 'Is Active']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_is_active = None\n",
    "            \n",
    "            for col in is_active_columns:\n",
    "                if col in result_df.columns:\n",
    "                    is_active_val = row.get(col)\n",
    "                    if (pd.notna(is_active_val) and \n",
    "                        is_active_val != '' and \n",
    "                        str(is_active_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_is_active = self.standardize_is_active(is_active_val)\n",
    "                        if standardized_is_active is not None:\n",
    "                            final_is_active = standardized_is_active\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_is_active'] = final_is_active\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def standardize_is_active(self, is_active_value) -> Optional[bool]:\n",
    "        if pd.isna(is_active_value) or is_active_value is None or is_active_value == '':\n",
    "            return None\n",
    "        \n",
    "        value_str = str(is_active_value).strip().lower()\n",
    "        \n",
    "        if value_str in ['nat', 'none', 'null', '']:\n",
    "            return None\n",
    "        \n",
    "        if value_str in ['true', 't', 'yes', 'y', '1', 'active', 'on']:\n",
    "            return True\n",
    "        elif value_str in ['false', 'f', 'no', 'n', '0', 'inactive', 'off']:\n",
    "            return False\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def create_clean_dataset(self) -> pd.DataFrame:\n",
    "        consolidated_df = self.consolidate_account_and_email()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_created_date()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_lead_source()\n",
    "    \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_opportunity_amount()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_is_active()\n",
    "        \n",
    "        clean_df = consolidated_df.copy()\n",
    "        \n",
    "        clean_df['account_name'] = consolidated_df['consolidated_account_name']\n",
    "        clean_df['contact_email'] = consolidated_df['consolidated_contact_email']\n",
    "        clean_df['created_date'] = consolidated_df['consolidated_created_date']\n",
    "        clean_df['lead_source'] = consolidated_df['consolidated_lead_source']\n",
    "        clean_df['opportunity_amount'] = consolidated_df['consolidated_opportunity_amount']\n",
    "        clean_df['is_active'] = consolidated_df['consolidated_is_active']\n",
    "        \n",
    "        columns_to_drop = [\n",
    "            'consolidated_account_name', 'consolidated_contact_email', 'consolidated_created_date',\n",
    "            'consolidated_lead_source', 'consolidated_opportunity_amount', 'consolidated_is_active',\n",
    "            'Account Name', 'AccountName', 'Contact Email', 'Created Date', 'Lead Source', \n",
    "            'Opportunity Amount', 'Is Active'\n",
    "        ]\n",
    "        \n",
    "        columns_to_drop = [col for col in columns_to_drop if col in clean_df.columns]\n",
    "        clean_df = clean_df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        return clean_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing format_date_to_standard:\n",
      "'2023-01-15': 2023-01-15 (should be 2023-01-15)\n",
      "'01/15/2023': 2023-01-15 (should be 2023-01-15)\n",
      "'15-01-2023': 2023-01-15 (should be 2023-01-15)\n",
      "'2023/01/15': 2023-01-15 (should be 2023-01-15)\n",
      "'Jan 15, 2023': 2023-01-15 (should be 2023-01-15)\n",
      "'15 January 2023': 2023-01-15 (should be 2023-01-15)\n",
      "'2023-1-5': 2023-01-05 (should be 2023-01-05)\n",
      "None: None (should be None)\n",
      "empty string: None (should be None)\n",
      "'invalid_date': None (should be None)\n",
      "'2023-13-45': None (should be None - invalid date)\n",
      "'not a date': None (should be None)\n",
      "\n",
      "Testing is_valid_date_format:\n",
      "'2023-01-15': True (should be True)\n",
      "'2023-12-31': True (should be True)\n",
      "'2023-1-5': False (should be False - wrong format)\n",
      "'01/15/2023': False (should be False - wrong format)\n",
      "'2023/01/15': False (should be False - wrong format)\n",
      "'Jan 15, 2023': False (should be False - wrong format)\n",
      "'2023-13-45': False (should be False - invalid date)\n",
      "None: False (should be False)\n",
      "empty string: False (should be False)\n",
      "'not a date': False (should be False)\n",
      "'2023-02-29': False (should be False - not a leap year)\n",
      "'2024-02-29': True (should be True - leap year)\n",
      "\n",
      "Testing edge cases:\n",
      "'2023-01-01': 2023-01-01 (should be 2023-01-01)\n",
      "'2023-12-31': 2023-12-31 (should be 2023-12-31)\n",
      "'1900-01-01': 1900-01-01 (should be 1900-01-01)\n",
      "'2100-12-31': 2100-12-31 (should be 2100-12-31)\n",
      "'02/29/2024': 2024-02-29 (should be 2024-02-29 - leap year)\n",
      "'02/29/2023': None (should be None - not leap year)\n",
      "123456789: 1970-01-01 (should handle numeric input)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/mgvcrgd90878wk8lmjdc_4pc0000gn/T/ipykernel_2237/3549134330.py:15: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  parsed_date = pd.to_datetime(date_value, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "class DateTester:\n",
    "    def format_date_to_standard(self, date_value) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Format date to standard YYYY-MM-DD format\n",
    "        Handles various input formats and returns None for invalid dates\n",
    "        \"\"\"\n",
    "        if pd.isna(date_value) or date_value is None or date_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Convert to pandas datetime which handles many formats automatically\n",
    "            parsed_date = pd.to_datetime(date_value, errors='coerce')\n",
    "            \n",
    "            # If parsing failed, return None\n",
    "            if pd.isna(parsed_date):\n",
    "                return None\n",
    "            \n",
    "            # Return in standard YYYY-MM-DD format\n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    def is_valid_date_format(self, date_string) -> bool:\n",
    "        \"\"\"\n",
    "        Check if date string is in the standard YYYY-MM-DD format\n",
    "        \"\"\"\n",
    "        if pd.isna(date_string) or date_string is None or date_string == '':\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Check if it matches YYYY-MM-DD pattern exactly\n",
    "            if len(str(date_string)) == 10 and str(date_string).count('-') == 2:\n",
    "                parts = str(date_string).split('-')\n",
    "                if (len(parts[0]) == 4 and len(parts[1]) == 2 and len(parts[2]) == 2 and\n",
    "                    parts[0].isdigit() and parts[1].isdigit() and parts[2].isdigit()):\n",
    "                    # Also verify it's a valid date\n",
    "                    pd.to_datetime(date_string, format='%Y-%m-%d', errors='raise')\n",
    "                    return True\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "dt = DateTester()\n",
    "\n",
    "# Test format_date_to_standard\n",
    "print(\"Testing format_date_to_standard:\")\n",
    "print(f\"'2023-01-15': {dt.format_date_to_standard('2023-01-15')} (should be 2023-01-15)\")\n",
    "print(f\"'01/15/2023': {dt.format_date_to_standard('01/15/2023')} (should be 2023-01-15)\")\n",
    "print(f\"'15-01-2023': {dt.format_date_to_standard('15-01-2023')} (should be 2023-01-15)\")\n",
    "print(f\"'2023/01/15': {dt.format_date_to_standard('2023/01/15')} (should be 2023-01-15)\")\n",
    "print(f\"'Jan 15, 2023': {dt.format_date_to_standard('Jan 15, 2023')} (should be 2023-01-15)\")\n",
    "print(f\"'15 January 2023': {dt.format_date_to_standard('15 January 2023')} (should be 2023-01-15)\")\n",
    "print(f\"'2023-1-5': {dt.format_date_to_standard('2023-1-5')} (should be 2023-01-05)\")\n",
    "print(f\"None: {dt.format_date_to_standard(None)} (should be None)\")\n",
    "print(f\"empty string: {dt.format_date_to_standard('')} (should be None)\")\n",
    "print(f\"'invalid_date': {dt.format_date_to_standard('invalid_date')} (should be None)\")\n",
    "print(f\"'2023-13-45': {dt.format_date_to_standard('2023-13-45')} (should be None - invalid date)\")\n",
    "print(f\"'not a date': {dt.format_date_to_standard('not a date')} (should be None)\")\n",
    "print()\n",
    "\n",
    "# Test is_valid_date_format\n",
    "print(\"Testing is_valid_date_format:\")\n",
    "print(f\"'2023-01-15': {dt.is_valid_date_format('2023-01-15')} (should be True)\")\n",
    "print(f\"'2023-12-31': {dt.is_valid_date_format('2023-12-31')} (should be True)\")\n",
    "print(f\"'2023-1-5': {dt.is_valid_date_format('2023-1-5')} (should be False - wrong format)\")\n",
    "print(f\"'01/15/2023': {dt.is_valid_date_format('01/15/2023')} (should be False - wrong format)\")\n",
    "print(f\"'2023/01/15': {dt.is_valid_date_format('2023/01/15')} (should be False - wrong format)\")\n",
    "print(f\"'Jan 15, 2023': {dt.is_valid_date_format('Jan 15, 2023')} (should be False - wrong format)\")\n",
    "print(f\"'2023-13-45': {dt.is_valid_date_format('2023-13-45')} (should be False - invalid date)\")\n",
    "print(f\"None: {dt.is_valid_date_format(None)} (should be False)\")\n",
    "print(f\"empty string: {dt.is_valid_date_format('')} (should be False)\")\n",
    "print(f\"'not a date': {dt.is_valid_date_format('not a date')} (should be False)\")\n",
    "print(f\"'2023-02-29': {dt.is_valid_date_format('2023-02-29')} (should be False - not a leap year)\")\n",
    "print(f\"'2024-02-29': {dt.is_valid_date_format('2024-02-29')} (should be True - leap year)\")\n",
    "print()\n",
    "\n",
    "# Test edge cases\n",
    "print(\"Testing edge cases:\")\n",
    "print(f\"'2023-01-01': {dt.format_date_to_standard('2023-01-01')} (should be 2023-01-01)\")\n",
    "print(f\"'2023-12-31': {dt.format_date_to_standard('2023-12-31')} (should be 2023-12-31)\")\n",
    "print(f\"'1900-01-01': {dt.format_date_to_standard('1900-01-01')} (should be 1900-01-01)\")\n",
    "print(f\"'2100-12-31': {dt.format_date_to_standard('2100-12-31')} (should be 2100-12-31)\")\n",
    "print(f\"'02/29/2024': {dt.format_date_to_standard('02/29/2024')} (should be 2024-02-29 - leap year)\")\n",
    "print(f\"'02/29/2023': {dt.format_date_to_standard('02/29/2023')} (should be None - not leap year)\")\n",
    "print(f\"123456789: {dt.format_date_to_standard(123456789)} (should handle numeric input)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Info:\n",
      "Shape: (500, 30)\n",
      "Columns: ['Account Name', 'account_name', 'AccountName', 'Contact Email', 'contact_email', 'Created Date', 'created_date', 'Lead Source', 'lead_source', 'Opportunity Amount', 'opportunity_amount', 'Is Active', 'is_active', 'SFDC ID', 'sfdc_id', 'Annual Revenue', 'annual_revenue', 'Last Activity', 'Custom Field', 'Region', 'Unnamed: 0', 'Unnamed: 21', 'Random Notes', 'Deal Score', 'Engagement Level', 'Num Calls', 'Time on Page (sec)', 'City', 'State', 'Country']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Cleansing' object has no attribute 'consolidate_is_active'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m cleaner \u001b[38;5;241m=\u001b[39m Cleansing(df)\n\u001b[0;32m----> 8\u001b[0m cleaner\u001b[38;5;241m.\u001b[39mcreate_clean_dataset()\n\u001b[1;32m     10\u001b[0m clean_df \u001b[38;5;241m=\u001b[39m cleaner\u001b[38;5;241m.\u001b[39mcreate_clean_dataset()\n\u001b[1;32m     12\u001b[0m clean_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleanSalesforceData.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[36], line 334\u001b[0m, in \u001b[0;36mCleansing.create_clean_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m consolidated_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsolidate_opportunity_amount()\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m consolidated_df\n\u001b[0;32m--> 334\u001b[0m consolidated_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsolidate_is_active()\n\u001b[1;32m    336\u001b[0m clean_df \u001b[38;5;241m=\u001b[39m consolidated_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    338\u001b[0m clean_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccount_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m consolidated_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconsolidated_account_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Cleansing' object has no attribute 'consolidate_is_active'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/DirtySalesforceData.csv')\n",
    "\n",
    "print(\"Original Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "cleaner = Cleansing(df)\n",
    "cleaner.create_clean_dataset()\n",
    "\n",
    "clean_df = cleaner.create_clean_dataset()\n",
    "\n",
    "clean_df.to_csv('CleanSalesforceData.csv', index=False)\n",
    "print(f\"\\nClean dataset saved as 'CleanSalesforceData.csv'\")\n",
    "print(f\"Clean dataset shape: {clean_df.shape}\")\n",
    "\n",
    "print(f\"\\nSample of consolidated data:\")\n",
    "print(clean_df.head(30))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
