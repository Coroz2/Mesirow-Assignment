{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleansing:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        self.duplicate_groups = {\n",
    "            'account_name': ['Account Name', 'account_name', 'AccountName'],\n",
    "            'contact_email': ['Contact Email', 'contact_email'],\n",
    "            'created_date': ['Created Date', 'created_date'],\n",
    "            'lead_source': ['Lead Source', 'lead_source'],\n",
    "            'opportunity_amount': ['Opportunity Amount', 'opportunity_amount'],\n",
    "            'is_active': ['Is Active', 'is_active'],\n",
    "            'sfdc_id': ['SFDC ID', 'sfdc_id'],\n",
    "            'annual_revenue': ['Annual Revenue', 'annual_revenue']\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def is_valid_email(self, email: str) -> bool:\n",
    "        # Check if email is one of the valid emails\n",
    "        if pd.isna(email) or email is None:\n",
    "            return False\n",
    "        return email in ['help@globex.com', 'contact@acme.com']\n",
    "    \n",
    "    def is_placeholder_email(self, email: str) -> bool:\n",
    "        # Check if email is a placeholder \n",
    "        if pd.isna(email) or email is None or email == '':\n",
    "            return True\n",
    "        placeholders = ['noemail', 'invalid@', 'user@', 'missing.com', 'placeholder']\n",
    "        return any(placeholder in str(email).lower() for placeholder in placeholders)\n",
    "    \n",
    "    def get_corresponding_account(self, email: str) -> Optional[str]:\n",
    "        # Get the corresponding account name for a valid email\n",
    "        if email == 'help@globex.com':\n",
    "            return 'Globex'\n",
    "        elif email == 'contact@acme.com':\n",
    "            return 'Acme Corp'\n",
    "        return None\n",
    "    \n",
    "    def generate_email_for_account(self, account_name: str) -> Optional[str]:\n",
    "        # Generate corresponding email for Globex or Acme Corp\n",
    "        if account_name == 'Globex':\n",
    "            return 'help@globex.com'\n",
    "        elif account_name == 'Acme Corp':\n",
    "            return 'contact@acme.com'\n",
    "        return None\n",
    "    \n",
    "    def consolidate_account_and_email(self) -> pd.DataFrame:\n",
    "\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_account_name'] = None\n",
    "        result_df['consolidated_contact_email'] = None\n",
    "        \n",
    "        account_columns = ['account_name', 'AccountName', 'Account Name']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "\n",
    "            contact_email_lower_val = row.get('contact_email')\n",
    "            contact_email_val = row.get('Contact Email')\n",
    "            \n",
    "            contact_email_lower_valid = self.is_valid_email(contact_email_lower_val)\n",
    "            contact_email_valid = self.is_valid_email(contact_email_val)\n",
    "            \n",
    "            account_values = {}\n",
    "            for col in account_columns:\n",
    "                if col in result_df.columns:\n",
    "                    account_values[col] = row.get(col)\n",
    "            \n",
    "            final_account = None\n",
    "            final_email = None\n",
    "            \n",
    "            for col in account_columns:\n",
    "                if (col in account_values and \n",
    "                    pd.notna(account_values[col]) and \n",
    "                    account_values[col] != ''):\n",
    "                    final_account = account_values[col]\n",
    "                    break\n",
    "            \n",
    "            # If no account found, derive from valid email (all accounts empty case)\n",
    "            if final_account is None:\n",
    "                # Check contact_email first (priority)\n",
    "                if contact_email_lower_valid:\n",
    "                    final_account = self.get_corresponding_account(contact_email_lower_val)\n",
    "                    final_email = contact_email_lower_val\n",
    "                # If contact_email not valid, check Contact Email\n",
    "                elif contact_email_valid:\n",
    "                    final_account = self.get_corresponding_account(contact_email_val)\n",
    "                    final_email = contact_email_val\n",
    "            \n",
    "            if final_account is not None and final_email is None:\n",
    "                email_matched = False\n",
    "                \n",
    "                if contact_email_lower_valid:\n",
    "                    expected_account = self.get_corresponding_account(contact_email_lower_val)\n",
    "                    if final_account == expected_account:\n",
    "                        final_email = contact_email_lower_val\n",
    "                        email_matched = True\n",
    "                \n",
    "                if not email_matched and contact_email_valid:\n",
    "                    expected_account = self.get_corresponding_account(contact_email_val)\n",
    "                    if final_account == expected_account:\n",
    "                        final_email = contact_email_val\n",
    "                        email_matched = True\n",
    "                \n",
    "                # If no email match but account is Globex or Acme Corp, generate email\n",
    "                if not email_matched:\n",
    "                    generated_email = self.generate_email_for_account(final_account)\n",
    "                    if generated_email:\n",
    "                        final_email = generated_email\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_account_name'] = final_account\n",
    "            result_df.at[idx, 'consolidated_contact_email'] = final_email\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def consolidate_created_date(self) -> pd.DataFrame:\n",
    "    \n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_created_date'] = None\n",
    "        \n",
    "        created_date_columns = ['created_date', 'Created Date']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_created_date = None\n",
    "            \n",
    "            for col in created_date_columns:\n",
    "                if col in result_df.columns:\n",
    "                    raw_date = row.get(col)\n",
    "                    if (pd.isna(raw_date) or raw_date == '' or \n",
    "                        str(raw_date).lower() in ['nat', 'not_a_date', 'none']):\n",
    "                        continue\n",
    "                    \n",
    "                    formatted_date = self.format_date_to_standard(raw_date)\n",
    "                    if formatted_date:\n",
    "                        final_created_date = formatted_date\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_created_date'] = final_created_date\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def format_date_to_standard(self, date_value) -> Optional[str]:\n",
    "        if pd.isna(date_value) or date_value is None or date_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            parsed_date = pd.to_datetime(date_value, errors='coerce')\n",
    "            \n",
    "            if pd.isna(parsed_date):\n",
    "                return None\n",
    "            \n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    def consolidate_lead_source(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_lead_source'] = None\n",
    "        \n",
    "        lead_source_columns = ['lead_source', 'Lead Source']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_lead_source = None\n",
    "            \n",
    "            for col in lead_source_columns:\n",
    "                if col in result_df.columns:\n",
    "                    lead_source_val = row.get(col)\n",
    "                    if (pd.notna(lead_source_val) and \n",
    "                        lead_source_val != '' and \n",
    "                        str(lead_source_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        final_lead_source = lead_source_val\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_lead_source'] = final_lead_source\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_is_active(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_is_active'] = None\n",
    "        \n",
    "        is_active_columns = ['is_active', 'Is Active']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_is_active = None\n",
    "            \n",
    "            for col in is_active_columns:\n",
    "                if col in result_df.columns:\n",
    "                    is_active_val = row.get(col)\n",
    "                    if (pd.notna(is_active_val) and \n",
    "                        is_active_val != '' and \n",
    "                        str(is_active_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_is_active = self.standardize_is_active(is_active_val)\n",
    "                        if standardized_is_active is not None:\n",
    "                            final_is_active = standardized_is_active\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_is_active'] = final_is_active\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def standardize_is_active(self, is_active_value) -> Optional[bool]:\n",
    "        if pd.isna(is_active_value) or is_active_value is None or is_active_value == '':\n",
    "            return None\n",
    "        \n",
    "        value_str = str(is_active_value).strip().lower()\n",
    "        \n",
    "        if value_str in ['nat', 'none', 'null', '']:\n",
    "            return None\n",
    "        \n",
    "        if value_str in ['true', 't', 'yes', 'y', '1', 'active', 'on']:\n",
    "            return True\n",
    "        elif value_str in ['false', 'f', 'no', 'n', '0', 'inactive', 'off']:\n",
    "            return False\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def consolidate_sfdc_id(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_sfdc_id'] = None\n",
    "        \n",
    "        sfdc_id_columns = ['SFDC ID', 'sfdc_id']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_sfdc_id = None\n",
    "            \n",
    "            for col in sfdc_id_columns:\n",
    "                if col in result_df.columns:\n",
    "                    sfdc_id_val = row.get(col)\n",
    "                    if (pd.notna(sfdc_id_val) and \n",
    "                        sfdc_id_val != '' and \n",
    "                        str(sfdc_id_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_sfdc_id = self.standardize_sfdc_id(sfdc_id_val)\n",
    "                        if standardized_sfdc_id is not None:\n",
    "                            final_sfdc_id = standardized_sfdc_id\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_sfdc_id'] = final_sfdc_id\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def standardize_sfdc_id(self, sfdc_id_value) -> Optional[str]:\n",
    "        if pd.isna(sfdc_id_value) or sfdc_id_value is None or sfdc_id_value == '':\n",
    "            return None\n",
    "        \n",
    "        value_str = str(sfdc_id_value).strip()\n",
    "        \n",
    "        if value_str.lower() in ['nat', 'none', 'null', '']:\n",
    "            return None\n",
    "        \n",
    "        # Check for placeholder values\n",
    "        placeholder_patterns = ['abc123', 'xyz-00001', '12345', 'bad_id']\n",
    "        if value_str.lower() in placeholder_patterns:\n",
    "            return None\n",
    "        \n",
    "        return value_str\n",
    "    \n",
    "    def consolidate_monetary(self, field_name: str) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        consolidated_column = f'consolidated_{field_name}'\n",
    "        result_df[consolidated_column] = None\n",
    "        \n",
    "        field_columns = self.duplicate_groups.get(field_name, [field_name])\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_value = None\n",
    "            \n",
    "            for col in field_columns:\n",
    "                if col in result_df.columns:\n",
    "                    raw_value = row.get(col)\n",
    "                    if (pd.notna(raw_value) and \n",
    "                        raw_value != '' and \n",
    "                        str(raw_value).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_value = self.standardize_monetary(raw_value)\n",
    "                        if standardized_value is not None:\n",
    "                            final_value = standardized_value\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, consolidated_column] = final_value\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def standardize_monetary(self, monetary_value) -> Optional[float]:\n",
    "        if pd.isna(monetary_value) or monetary_value is None or monetary_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            value_str = str(monetary_value).strip()\n",
    "            \n",
    "            if value_str.lower() in ['nat', 'none', 'null', '', 'not available', 'n/a']:\n",
    "                return None\n",
    "            \n",
    "            # Check if it's already a valid number\n",
    "            if isinstance(monetary_value, (int, float)) and not pd.isna(monetary_value):\n",
    "                if monetary_value < 0:\n",
    "                    return None\n",
    "                return round(float(monetary_value), 2)\n",
    "            \n",
    "            # Word version\n",
    "            if re.match(r'^[a-zA-Z\\s]+$', value_str):\n",
    "                converted_float = self.convert_text_to_number(value_str)\n",
    "                if converted_float is None:\n",
    "                    return None\n",
    "                if converted_float < 0:\n",
    "                    return None\n",
    "                return round(converted_float, 2)\n",
    "            \n",
    "            # Remove currency symbols and common formatting for numeric values\n",
    "            value_str = re.sub(r'[$£€¥₹]', '', value_str)\n",
    "            value_str = value_str.replace(',', '').replace(' ', '') \n",
    "            \n",
    "            if value_str == '':\n",
    "                return None\n",
    "            \n",
    "            multiplier = 1\n",
    "            value_str_lower = value_str.lower()\n",
    "            if value_str_lower.endswith('k'):\n",
    "                multiplier = 1000\n",
    "                value_str = value_str[:-1]\n",
    "            elif value_str_lower.endswith('m'):\n",
    "                multiplier = 1000000\n",
    "                value_str = value_str[:-1]\n",
    "            elif value_str_lower.endswith('b'):\n",
    "                multiplier = 1000000000\n",
    "                value_str = value_str[:-1]\n",
    "            \n",
    "            try:\n",
    "                final_float = float(value_str) * multiplier\n",
    "            except ValueError:\n",
    "                return None\n",
    "            \n",
    "            if final_float < 0:\n",
    "                return None\n",
    "            \n",
    "            return round(final_float, 2)\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    def convert_text_to_number(self, text_value: str) -> Optional[float]:\n",
    "        if not text_value:\n",
    "            return None\n",
    "            \n",
    "        text_value = text_value.lower().strip()\n",
    "        \n",
    "        number_words = {\n",
    "            'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,\n",
    "            'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10,\n",
    "            'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15,\n",
    "            'sixteen': 16, 'seventeen': 17, 'eighteen': 18, 'nineteen': 19, 'twenty': 20,\n",
    "            'thirty': 30, 'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70,\n",
    "            'eighty': 80, 'ninety': 90, 'hundred': 100, 'thousand': 1000, 'million': 1000000,\n",
    "            'billion': 1000000000\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            words = text_value.split()\n",
    "            total = 0\n",
    "            current = 0\n",
    "            found_valid_word = False\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip()\n",
    "                if word in number_words:\n",
    "                    found_valid_word = True\n",
    "                    value = number_words[word]\n",
    "                    if value == 100:\n",
    "                        current = current * 100 if current > 0 else 100\n",
    "                    elif value >= 1000:\n",
    "                        total += current * value\n",
    "                        current = 0\n",
    "                    else:\n",
    "                        current += value\n",
    "            \n",
    "            total += current\n",
    "            \n",
    "            if not found_valid_word or (total == 0 and 'zero' not in text_value):\n",
    "                return None\n",
    "            \n",
    "            return float(total)\n",
    "        \n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def create_clean_dataset(self) -> pd.DataFrame:\n",
    "        consolidated_df = self.consolidate_account_and_email()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_created_date()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_lead_source()\n",
    "    \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_monetary('opportunity_amount')\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_is_active()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_sfdc_id()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_monetary('annual_revenue')\n",
    "        \n",
    "        clean_df = consolidated_df.copy()\n",
    "        \n",
    "        clean_df['account_name'] = consolidated_df['consolidated_account_name']\n",
    "        clean_df['contact_email'] = consolidated_df['consolidated_contact_email']\n",
    "        clean_df['created_date'] = consolidated_df['consolidated_created_date']\n",
    "        clean_df['lead_source'] = consolidated_df['consolidated_lead_source']\n",
    "        clean_df['opportunity_amount'] = consolidated_df['consolidated_opportunity_amount']\n",
    "        clean_df['is_active'] = consolidated_df['consolidated_is_active']\n",
    "        clean_df['sfdc_id'] = consolidated_df['consolidated_sfdc_id']\n",
    "        clean_df['annual_revenue'] = consolidated_df['consolidated_annual_revenue']\n",
    "        \n",
    "        \n",
    "        columns_to_drop = [\n",
    "            'consolidated_account_name', 'consolidated_contact_email', 'consolidated_created_date',\n",
    "            'consolidated_lead_source', 'consolidated_opportunity_amount', 'consolidated_is_active',\n",
    "            'consolidated_sfdc_id', 'consolidated_annual_revenue', 'Account Name', 'AccountName', 'Contact Email', 'Created Date', 'Lead Source', \n",
    "            'Opportunity Amount', 'Is Active', 'SFDC ID', 'Annual Revenue'\n",
    "        ]\n",
    "        \n",
    "        columns_to_drop = [col for col in columns_to_drop if col in clean_df.columns]\n",
    "        clean_df = clean_df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        return clean_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing format_date_to_standard:\n",
      "'2023-01-15': 2023-01-15 (should be 2023-01-15)\n",
      "'01/15/2023': 2023-01-15 (should be 2023-01-15)\n",
      "'15-01-2023': 2023-01-15 (should be 2023-01-15)\n",
      "'2023/01/15': 2023-01-15 (should be 2023-01-15)\n",
      "'Jan 15, 2023': 2023-01-15 (should be 2023-01-15)\n",
      "'15 January 2023': 2023-01-15 (should be 2023-01-15)\n",
      "'2023-1-5': 2023-01-05 (should be 2023-01-05)\n",
      "None: None (should be None)\n",
      "empty string: None (should be None)\n",
      "'invalid_date': None (should be None)\n",
      "'2023-13-45': None (should be None - invalid date)\n",
      "'not a date': None (should be None)\n",
      "\n",
      "Testing is_valid_date_format:\n",
      "'2023-01-15': True (should be True)\n",
      "'2023-12-31': True (should be True)\n",
      "'2023-1-5': False (should be False - wrong format)\n",
      "'01/15/2023': False (should be False - wrong format)\n",
      "'2023/01/15': False (should be False - wrong format)\n",
      "'Jan 15, 2023': False (should be False - wrong format)\n",
      "'2023-13-45': False (should be False - invalid date)\n",
      "None: False (should be False)\n",
      "empty string: False (should be False)\n",
      "'not a date': False (should be False)\n",
      "'2023-02-29': False (should be False - not a leap year)\n",
      "'2024-02-29': True (should be True - leap year)\n",
      "\n",
      "Testing edge cases:\n",
      "'2023-01-01': 2023-01-01 (should be 2023-01-01)\n",
      "'2023-12-31': 2023-12-31 (should be 2023-12-31)\n",
      "'1900-01-01': 1900-01-01 (should be 1900-01-01)\n",
      "'2100-12-31': 2100-12-31 (should be 2100-12-31)\n",
      "'02/29/2024': 2024-02-29 (should be 2024-02-29 - leap year)\n",
      "'02/29/2023': None (should be None - not leap year)\n",
      "123456789: 1970-01-01 (should handle numeric input)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/mgvcrgd90878wk8lmjdc_4pc0000gn/T/ipykernel_2237/3549134330.py:15: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  parsed_date = pd.to_datetime(date_value, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "class DateTester:\n",
    "    def format_date_to_standard(self, date_value) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Format date to standard YYYY-MM-DD format\n",
    "        Handles various input formats and returns None for invalid dates\n",
    "        \"\"\"\n",
    "        if pd.isna(date_value) or date_value is None or date_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Convert to pandas datetime which handles many formats automatically\n",
    "            parsed_date = pd.to_datetime(date_value, errors='coerce')\n",
    "            \n",
    "            # If parsing failed, return None\n",
    "            if pd.isna(parsed_date):\n",
    "                return None\n",
    "            \n",
    "            # Return in standard YYYY-MM-DD format\n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    def is_valid_date_format(self, date_string) -> bool:\n",
    "        \"\"\"\n",
    "        Check if date string is in the standard YYYY-MM-DD format\n",
    "        \"\"\"\n",
    "        if pd.isna(date_string) or date_string is None or date_string == '':\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Check if it matches YYYY-MM-DD pattern exactly\n",
    "            if len(str(date_string)) == 10 and str(date_string).count('-') == 2:\n",
    "                parts = str(date_string).split('-')\n",
    "                if (len(parts[0]) == 4 and len(parts[1]) == 2 and len(parts[2]) == 2 and\n",
    "                    parts[0].isdigit() and parts[1].isdigit() and parts[2].isdigit()):\n",
    "                    # Also verify it's a valid date\n",
    "                    pd.to_datetime(date_string, format='%Y-%m-%d', errors='raise')\n",
    "                    return True\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "dt = DateTester()\n",
    "\n",
    "# Test format_date_to_standard\n",
    "print(\"Testing format_date_to_standard:\")\n",
    "print(f\"'2023-01-15': {dt.format_date_to_standard('2023-01-15')} (should be 2023-01-15)\")\n",
    "print(f\"'01/15/2023': {dt.format_date_to_standard('01/15/2023')} (should be 2023-01-15)\")\n",
    "print(f\"'15-01-2023': {dt.format_date_to_standard('15-01-2023')} (should be 2023-01-15)\")\n",
    "print(f\"'2023/01/15': {dt.format_date_to_standard('2023/01/15')} (should be 2023-01-15)\")\n",
    "print(f\"'Jan 15, 2023': {dt.format_date_to_standard('Jan 15, 2023')} (should be 2023-01-15)\")\n",
    "print(f\"'15 January 2023': {dt.format_date_to_standard('15 January 2023')} (should be 2023-01-15)\")\n",
    "print(f\"'2023-1-5': {dt.format_date_to_standard('2023-1-5')} (should be 2023-01-05)\")\n",
    "print(f\"None: {dt.format_date_to_standard(None)} (should be None)\")\n",
    "print(f\"empty string: {dt.format_date_to_standard('')} (should be None)\")\n",
    "print(f\"'invalid_date': {dt.format_date_to_standard('invalid_date')} (should be None)\")\n",
    "print(f\"'2023-13-45': {dt.format_date_to_standard('2023-13-45')} (should be None - invalid date)\")\n",
    "print(f\"'not a date': {dt.format_date_to_standard('not a date')} (should be None)\")\n",
    "print()\n",
    "\n",
    "# Test is_valid_date_format\n",
    "print(\"Testing is_valid_date_format:\")\n",
    "print(f\"'2023-01-15': {dt.is_valid_date_format('2023-01-15')} (should be True)\")\n",
    "print(f\"'2023-12-31': {dt.is_valid_date_format('2023-12-31')} (should be True)\")\n",
    "print(f\"'2023-1-5': {dt.is_valid_date_format('2023-1-5')} (should be False - wrong format)\")\n",
    "print(f\"'01/15/2023': {dt.is_valid_date_format('01/15/2023')} (should be False - wrong format)\")\n",
    "print(f\"'2023/01/15': {dt.is_valid_date_format('2023/01/15')} (should be False - wrong format)\")\n",
    "print(f\"'Jan 15, 2023': {dt.is_valid_date_format('Jan 15, 2023')} (should be False - wrong format)\")\n",
    "print(f\"'2023-13-45': {dt.is_valid_date_format('2023-13-45')} (should be False - invalid date)\")\n",
    "print(f\"None: {dt.is_valid_date_format(None)} (should be False)\")\n",
    "print(f\"empty string: {dt.is_valid_date_format('')} (should be False)\")\n",
    "print(f\"'not a date': {dt.is_valid_date_format('not a date')} (should be False)\")\n",
    "print(f\"'2023-02-29': {dt.is_valid_date_format('2023-02-29')} (should be False - not a leap year)\")\n",
    "print(f\"'2024-02-29': {dt.is_valid_date_format('2024-02-29')} (should be True - leap year)\")\n",
    "print()\n",
    "\n",
    "# Test edge cases\n",
    "print(\"Testing edge cases:\")\n",
    "print(f\"'2023-01-01': {dt.format_date_to_standard('2023-01-01')} (should be 2023-01-01)\")\n",
    "print(f\"'2023-12-31': {dt.format_date_to_standard('2023-12-31')} (should be 2023-12-31)\")\n",
    "print(f\"'1900-01-01': {dt.format_date_to_standard('1900-01-01')} (should be 1900-01-01)\")\n",
    "print(f\"'2100-12-31': {dt.format_date_to_standard('2100-12-31')} (should be 2100-12-31)\")\n",
    "print(f\"'02/29/2024': {dt.format_date_to_standard('02/29/2024')} (should be 2024-02-29 - leap year)\")\n",
    "print(f\"'02/29/2023': {dt.format_date_to_standard('02/29/2023')} (should be None - not leap year)\")\n",
    "print(f\"123456789: {dt.format_date_to_standard(123456789)} (should handle numeric input)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Info:\n",
      "Shape: (500, 30)\n",
      "Columns: ['Account Name', 'account_name', 'AccountName', 'Contact Email', 'contact_email', 'Created Date', 'created_date', 'Lead Source', 'lead_source', 'Opportunity Amount', 'opportunity_amount', 'Is Active', 'is_active', 'SFDC ID', 'sfdc_id', 'Annual Revenue', 'annual_revenue', 'Last Activity', 'Custom Field', 'Region', 'Unnamed: 0', 'Unnamed: 21', 'Random Notes', 'Deal Score', 'Engagement Level', 'Num Calls', 'Time on Page (sec)', 'City', 'State', 'Country']\n",
      "\n",
      "Clean dataset saved as 'CleanSalesforceData.csv'\n",
      "Clean dataset shape: (500, 21)\n",
      "\n",
      "Sample of consolidated data:\n",
      "   account_name     contact_email created_date     lead_source  \\\n",
      "0     Acme Corp  contact@acme.com   2020-01-01      Trade Show   \n",
      "1        Globex   help@globex.com   2022-12-01    Social Media   \n",
      "2        Globex   help@globex.com         None  Email Campaign   \n",
      "3     Acme Corp  contact@acme.com         None    Social Media   \n",
      "4      Umbrella              None   2024-03-26             Web   \n",
      "5        Globex   help@globex.com   2024-04-03            None   \n",
      "6       Soylent              None   2025-07-17        Referral   \n",
      "7       Soylent              None   2023-12-17   Phone Inquiry   \n",
      "8     Acme Corp  contact@acme.com   2022-12-01      Trade Show   \n",
      "9     Acme Corp  contact@acme.com         None  Email Campaign   \n",
      "10     Umbrella              None   2024-06-03      Trade Show   \n",
      "11        Wonka              None   2025-07-08      Trade Show   \n",
      "12       Globex   help@globex.com   2022-12-01            None   \n",
      "13      Soylent              None   2020-01-01      Trade Show   \n",
      "14      Soylent              None   2020-01-01         Partner   \n",
      "15       Globex   help@globex.com   2023-01-26    Social Media   \n",
      "16    Acme Corp  contact@acme.com   2022-12-01        Referral   \n",
      "17       Globex   help@globex.com         None             Web   \n",
      "18      Soylent              None   2020-01-01    Social Media   \n",
      "19       Globex   help@globex.com         None  Email Campaign   \n",
      "20     Umbrella              None   2020-01-01    Social Media   \n",
      "21     Umbrella              None   2024-08-12        Referral   \n",
      "22        Wonka              None   2022-12-01      Trade Show   \n",
      "23       Globex   help@globex.com   2022-12-01  Email Campaign   \n",
      "24      Soylent              None   2020-01-01  Email Campaign   \n",
      "25       Globex   help@globex.com   2024-12-27    Social Media   \n",
      "26    Acme Corp  contact@acme.com   2022-12-01    Social Media   \n",
      "27      Soylent              None   2023-05-04  Email Campaign   \n",
      "28      Initech              None         None             Web   \n",
      "29      Soylent              None         None  Email Campaign   \n",
      "\n",
      "   opportunity_amount is_active     sfdc_id annual_revenue  \\\n",
      "0              1000.0     False  001B000002      1000000.0   \n",
      "1                None     False        None      1000000.0   \n",
      "2             50000.0      True  001A000001      5000000.0   \n",
      "3             50000.0     False        None      1000000.0   \n",
      "4             50000.0     False  001B000002      1000000.0   \n",
      "5             50000.0      True  001A000001       999999.0   \n",
      "6             10000.0     False  001A000001           None   \n",
      "7              1000.0      True        None           None   \n",
      "8              1000.0     False  001B000002      2500000.0   \n",
      "9             50000.0     False  001C000003      5000000.0   \n",
      "10            25000.0      True  001A000001      2500000.0   \n",
      "11            50000.0      True        None      5000000.0   \n",
      "12             1000.0      True        None      1000000.0   \n",
      "13            50000.0      True  001B000002      1000000.0   \n",
      "14            50000.0     False        None      1000000.0   \n",
      "15            50000.0      True        None       100000.0   \n",
      "16            50000.0      True  001C000003      1000000.0   \n",
      "17            25000.0      True        None      1000000.0   \n",
      "18            10000.0      None  001A000001      5000000.0   \n",
      "19               None     False  001A000001      5000000.0   \n",
      "20             1000.0     False        None      1000000.0   \n",
      "21            10000.0      True        None      1000000.0   \n",
      "22            50000.0     False  001B000002      2500000.0   \n",
      "23            50000.0      True  001B000002      1000000.0   \n",
      "24               None      True        None           None   \n",
      "25                0.0      None        None      1000000.0   \n",
      "26            10000.0      True        None      1000000.0   \n",
      "27               None      True        None       999999.0   \n",
      "28            50000.0     False  001C000003      2500000.0   \n",
      "29             1000.0      True  001A000001      5000000.0   \n",
      "\n",
      "                 Last Activity                 Custom Field  ... Unnamed: 0  \\\n",
      "0                          NaN  {\"type\": \"A\", \"value\": 100}  ...          0   \n",
      "1                           42                          NaN  ...          1   \n",
      "2                   2024-05-05               {\"type\": null}  ...          2   \n",
      "3   2024-09-13 19:37:11.167657  {\"type\": \"A\", \"value\": 100}  ...          3   \n",
      "4                   2024-05-05                          NaN  ...          4   \n",
      "5                           42  {\"type\": \"A\", \"value\": 100}  ...          5   \n",
      "6   2025-01-28 19:37:11.167664  {\"type\": \"B\", \"value\": 200}  ...          6   \n",
      "7                Called client  {\"type\": \"A\", \"value\": 100}  ...          7   \n",
      "8   2025-01-23 19:37:11.167671                          NaN  ...          8   \n",
      "9                          NaN                          NaN  ...          9   \n",
      "10                         NaN               {\"type\": null}  ...         10   \n",
      "11                  2024-05-05               {\"type\": null}  ...         11   \n",
      "12               Called client  {\"type\": \"B\", \"value\": 200}  ...         12   \n",
      "13                  2024-05-05  {\"type\": \"A\", \"value\": 100}  ...         13   \n",
      "14                         NaN  {\"type\": \"B\", \"value\": 200}  ...         14   \n",
      "15                          42                          NaN  ...         15   \n",
      "16               Called client  {\"type\": \"B\", \"value\": 200}  ...         16   \n",
      "17                         NaN                          NaN  ...         17   \n",
      "18  2024-08-22 19:37:11.167705               {\"type\": null}  ...         18   \n",
      "19                          42                          NaN  ...         19   \n",
      "20                          42  {\"type\": \"B\", \"value\": 200}  ...         20   \n",
      "21                          42  {\"type\": \"A\", \"value\": 100}  ...         21   \n",
      "22                          42                          NaN  ...         22   \n",
      "23                          42               {\"type\": null}  ...         23   \n",
      "24  2024-12-18 19:37:11.167726               {\"type\": null}  ...         24   \n",
      "25                          42                          NaN  ...         25   \n",
      "26               Called client                          NaN  ...         26   \n",
      "27               Called client                          NaN  ...         27   \n",
      "28                          42  {\"type\": \"A\", \"value\": 100}  ...         28   \n",
      "29                         NaN                          NaN  ...         29   \n",
      "\n",
      "    Unnamed: 21  Random Notes Deal Score  Engagement Level  Num Calls  \\\n",
      "0           NaN           NaN       50.0               NaN        NaN   \n",
      "1           NaN           NaN        NaN          0.063609       31.0   \n",
      "2           NaN           NaN        NaN          0.217333       38.0   \n",
      "3           NaN           NaN      100.0               NaN       32.0   \n",
      "4           NaN           NaN      100.0          0.537578        NaN   \n",
      "5           NaN           NaN        NaN          0.766073       23.0   \n",
      "6           NaN           NaN      100.0               NaN        3.0   \n",
      "7           NaN     See notes       10.0          0.903417       37.0   \n",
      "8           NaN           NaN       50.0          0.541393        NaN   \n",
      "9           NaN     See notes        NaN               NaN       10.0   \n",
      "10          NaN     See notes       75.0          0.030900       49.0   \n",
      "11          NaN     See notes      100.0          0.154336       13.0   \n",
      "12          NaN           NaN       75.0               NaN        NaN   \n",
      "13          NaN         Valid      100.0          0.785308       26.0   \n",
      "14          NaN     See notes      100.0          0.274271       17.0   \n",
      "15          NaN           NaN       10.0               NaN       21.0   \n",
      "16          NaN     See notes        NaN          0.899141        NaN   \n",
      "17          NaN     See notes      100.0          0.060635        9.0   \n",
      "18          NaN         Valid        NaN               NaN       41.0   \n",
      "19          NaN           NaN      100.0          0.280493       44.0   \n",
      "20          NaN           NaN      100.0          0.095622        NaN   \n",
      "21          NaN     See notes       75.0               NaN       41.0   \n",
      "22          NaN     See notes       75.0          0.780823       37.0   \n",
      "23          NaN     See notes       75.0          0.290363        1.0   \n",
      "24          NaN           NaN       10.0               NaN        NaN   \n",
      "25          NaN     See notes       50.0          0.894061        8.0   \n",
      "26          NaN         Valid        NaN          0.053003       43.0   \n",
      "27          NaN           NaN        NaN               NaN       28.0   \n",
      "28          NaN         Valid      100.0          0.940728        NaN   \n",
      "29          NaN           NaN      100.0          0.958253       19.0   \n",
      "\n",
      "    Time on Page (sec)           City       State        Country  \n",
      "0                  141        Chicago        N.Y.            NaN  \n",
      "1                  278            NaN          CA            NaN  \n",
      "2                  271        Chicago         NaN  United States  \n",
      "3                  318            NaN         NaN  United States  \n",
      "4                  399  San Francisco        N.Y.  United States  \n",
      "5                  383        Chicago    Illinois             us  \n",
      "6                  155            NaN    Illinois             us  \n",
      "7                  248       New York    Illinois            USA  \n",
      "8                  419       New York          IL  United States  \n",
      "9                  108       New York          NY             us  \n",
      "10                 182  San Francisco    Illinois  United States  \n",
      "11                 194        chicago  California             us  \n",
      "12                 279            NaN    Illinois           U.S.  \n",
      "13                  37            NaN          NY            NaN  \n",
      "14                 350            nyc          NY            NaN  \n",
      "15                 105        Chicago  California             us  \n",
      "16                 331        chicago          CA            NaN  \n",
      "17                 296            nyc          CA             us  \n",
      "18                 169            NaN  California           U.S.  \n",
      "19                 353            nyc          CA           U.S.  \n",
      "20                 360       New York         NaN           U.S.  \n",
      "21                 472            NaN         NaN            USA  \n",
      "22                  73  San Francisco        N.Y.  United States  \n",
      "23                 113       New York          NY             us  \n",
      "24                  21            nyc          IL             us  \n",
      "25                  50            NaN          IL             us  \n",
      "26                 205            NaN         NaN            USA  \n",
      "27                 107       New York  California           U.S.  \n",
      "28                 203        chicago        N.Y.             us  \n",
      "29                 134        chicago        N.Y.           U.S.  \n",
      "\n",
      "[30 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/DirtySalesforceData.csv')\n",
    "\n",
    "print(\"Original Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "cleaner = Cleansing(df)\n",
    "cleaner.create_clean_dataset()\n",
    "\n",
    "clean_df = cleaner.create_clean_dataset()\n",
    "\n",
    "clean_df.to_csv('CleanSalesforceData.csv', index=False)\n",
    "print(f\"\\nClean dataset saved as 'CleanSalesforceData.csv'\")\n",
    "print(f\"Clean dataset shape: {clean_df.shape}\")\n",
    "\n",
    "print(f\"\\nSample of consolidated data:\")\n",
    "print(clean_df.head(30))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
