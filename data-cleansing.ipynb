{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Cleaning and Gold Standard Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleansing:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        self.duplicate_groups = {\n",
    "            'account_name': ['Account Name', 'account_name', 'AccountName'],\n",
    "            'contact_email': ['Contact Email', 'contact_email'],\n",
    "            'created_date': ['Created Date', 'created_date'],\n",
    "            'lead_source': ['Lead Source', 'lead_source'],\n",
    "            'opportunity_amount': ['Opportunity Amount', 'opportunity_amount'],\n",
    "            'is_active': ['Is Active', 'is_active'],\n",
    "            'sfdc_id': ['SFDC ID', 'sfdc_id'],\n",
    "            'annual_revenue': ['Annual Revenue', 'annual_revenue']\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def is_valid_email(self, email: str) -> bool:\n",
    "        # Check if email is one of the valid emails\n",
    "        if pd.isna(email) or email is None:\n",
    "            return False\n",
    "        return email in ['help@globex.com', 'contact@acme.com']\n",
    "    \n",
    "    def is_placeholder_email(self, email: str) -> bool:\n",
    "        # Check if email is a placeholder \n",
    "        if pd.isna(email) or email is None or email == '':\n",
    "            return True\n",
    "        placeholders = ['noemail', 'invalid@', 'user@', 'missing.com', 'placeholder']\n",
    "        return any(placeholder in str(email).lower() for placeholder in placeholders)\n",
    "    \n",
    "    def get_corresponding_account(self, email: str) -> Optional[str]:\n",
    "        # Get the corresponding account name for a valid email\n",
    "        if email == 'help@globex.com':\n",
    "            return 'Globex'\n",
    "        elif email == 'contact@acme.com':\n",
    "            return 'Acme Corp'\n",
    "        return None\n",
    "    \n",
    "    def generate_email_for_account(self, account_name: str) -> Optional[str]:\n",
    "        # Generate corresponding email for Globex or Acme Corp\n",
    "        if account_name == 'Globex':\n",
    "            return 'help@globex.com'\n",
    "        elif account_name == 'Acme Corp':\n",
    "            return 'contact@acme.com'\n",
    "        return None\n",
    "    \n",
    "    def consolidate_account_and_email(self) -> pd.DataFrame:\n",
    "\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_account_name'] = None\n",
    "        result_df['consolidated_contact_email'] = None\n",
    "        \n",
    "        account_columns = ['account_name', 'AccountName', 'Account Name']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "\n",
    "            contact_email_lower_val = row.get('contact_email')\n",
    "            contact_email_val = row.get('Contact Email')\n",
    "            \n",
    "            contact_email_lower_valid = self.is_valid_email(contact_email_lower_val)\n",
    "            contact_email_valid = self.is_valid_email(contact_email_val)\n",
    "            \n",
    "            account_values = {}\n",
    "            for col in account_columns:\n",
    "                if col in result_df.columns:\n",
    "                    account_values[col] = row.get(col)\n",
    "            \n",
    "            final_account = None\n",
    "            final_email = None\n",
    "            \n",
    "            for col in account_columns:\n",
    "                if (col in account_values and \n",
    "                    pd.notna(account_values[col]) and \n",
    "                    account_values[col] != ''):\n",
    "                    final_account = account_values[col]\n",
    "                    break\n",
    "            \n",
    "            # If no account found, derive from valid email (all accounts empty case)\n",
    "            if final_account is None:\n",
    "                # Check contact_email first (priority)\n",
    "                if contact_email_lower_valid:\n",
    "                    final_account = self.get_corresponding_account(contact_email_lower_val)\n",
    "                    final_email = contact_email_lower_val\n",
    "                # If contact_email not valid, check Contact Email\n",
    "                elif contact_email_valid:\n",
    "                    final_account = self.get_corresponding_account(contact_email_val)\n",
    "                    final_email = contact_email_val\n",
    "            \n",
    "            if final_account is not None and final_email is None:\n",
    "                email_matched = False\n",
    "                \n",
    "                if contact_email_lower_valid:\n",
    "                    expected_account = self.get_corresponding_account(contact_email_lower_val)\n",
    "                    if final_account == expected_account:\n",
    "                        final_email = contact_email_lower_val\n",
    "                        email_matched = True\n",
    "                \n",
    "                if not email_matched and contact_email_valid:\n",
    "                    expected_account = self.get_corresponding_account(contact_email_val)\n",
    "                    if final_account == expected_account:\n",
    "                        final_email = contact_email_val\n",
    "                        email_matched = True\n",
    "                \n",
    "                # If no email match but account is Globex or Acme Corp, generate email\n",
    "                if not email_matched:\n",
    "                    generated_email = self.generate_email_for_account(final_account)\n",
    "                    if generated_email:\n",
    "                        final_email = generated_email\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_account_name'] = final_account\n",
    "            result_df.at[idx, 'consolidated_contact_email'] = final_email\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def consolidate_created_date(self) -> pd.DataFrame:\n",
    "    \n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_created_date'] = None\n",
    "        \n",
    "        created_date_columns = ['created_date', 'Created Date']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_created_date = None\n",
    "            \n",
    "            for col in created_date_columns:\n",
    "                if col in result_df.columns:\n",
    "                    raw_date = row.get(col)\n",
    "                    if (pd.isna(raw_date) or raw_date == '' or \n",
    "                        str(raw_date).lower() in ['nat', 'not_a_date', 'none']):\n",
    "                        continue\n",
    "                    \n",
    "                    formatted_date = self.format_date_to_standard(raw_date)\n",
    "                    if formatted_date:\n",
    "                        final_created_date = formatted_date\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_created_date'] = final_created_date\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def format_date_to_standard(self, date_value) -> Optional[str]:\n",
    "        if pd.isna(date_value) or date_value is None or date_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            parsed_date = pd.to_datetime(date_value, errors='coerce')\n",
    "            \n",
    "            if pd.isna(parsed_date):\n",
    "                return None\n",
    "            \n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    def consolidate_lead_source(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_lead_source'] = None\n",
    "        \n",
    "        lead_source_columns = ['lead_source', 'Lead Source']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_lead_source = None\n",
    "            \n",
    "            for col in lead_source_columns:\n",
    "                if col in result_df.columns:\n",
    "                    lead_source_val = row.get(col)\n",
    "                    if (pd.notna(lead_source_val) and \n",
    "                        lead_source_val != '' and \n",
    "                        str(lead_source_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        final_lead_source = lead_source_val\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_lead_source'] = final_lead_source\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_is_active(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_is_active'] = None\n",
    "        \n",
    "        is_active_columns = ['is_active', 'Is Active']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_is_active = None\n",
    "            \n",
    "            for col in is_active_columns:\n",
    "                if col in result_df.columns:\n",
    "                    is_active_val = row.get(col)\n",
    "                    if (pd.notna(is_active_val) and \n",
    "                        is_active_val != '' and \n",
    "                        str(is_active_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_is_active = self.standardize_is_active(is_active_val)\n",
    "                        if standardized_is_active is not None:\n",
    "                            final_is_active = standardized_is_active\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_is_active'] = final_is_active\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def standardize_is_active(self, is_active_value) -> Optional[bool]:\n",
    "        if pd.isna(is_active_value) or is_active_value is None or is_active_value == '':\n",
    "            return None\n",
    "        \n",
    "        value_str = str(is_active_value).strip().lower()\n",
    "        \n",
    "        if value_str in ['nat', 'none', 'null', '']:\n",
    "            return None\n",
    "        \n",
    "        if value_str in ['true', 't', 'yes', 'y', '1', 'active', 'on']:\n",
    "            return True\n",
    "        elif value_str in ['false', 'f', 'no', 'n', '0', 'inactive', 'off']:\n",
    "            return False\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def consolidate_sfdc_id(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_sfdc_id'] = None\n",
    "        \n",
    "        sfdc_id_columns = ['SFDC ID', 'sfdc_id']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_sfdc_id = None\n",
    "            \n",
    "            for col in sfdc_id_columns:\n",
    "                if col in result_df.columns:\n",
    "                    sfdc_id_val = row.get(col)\n",
    "                    if (pd.notna(sfdc_id_val) and \n",
    "                        sfdc_id_val != '' and \n",
    "                        str(sfdc_id_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_sfdc_id = self.standardize_sfdc_id(sfdc_id_val)\n",
    "                        if standardized_sfdc_id is not None:\n",
    "                            final_sfdc_id = standardized_sfdc_id\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_sfdc_id'] = final_sfdc_id\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def standardize_sfdc_id(self, sfdc_id_value) -> Optional[str]:\n",
    "        if pd.isna(sfdc_id_value) or sfdc_id_value is None or sfdc_id_value == '':\n",
    "            return None\n",
    "        \n",
    "        value_str = str(sfdc_id_value).strip()\n",
    "        \n",
    "        if value_str.lower() in ['nat', 'none', 'null', '']:\n",
    "            return None\n",
    "        \n",
    "        # Check for placeholder values\n",
    "        placeholder_patterns = ['abc123', 'xyz-00001', '12345', 'bad_id']\n",
    "        if value_str.lower() in placeholder_patterns:\n",
    "            return None\n",
    "        \n",
    "        return value_str\n",
    "    \n",
    "    def consolidate_monetary(self, field_name: str) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        consolidated_column = f'consolidated_{field_name}'\n",
    "        result_df[consolidated_column] = None\n",
    "        \n",
    "        field_columns = self.duplicate_groups.get(field_name, [field_name])\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_value = None\n",
    "            \n",
    "            for col in field_columns:\n",
    "                if col in result_df.columns:\n",
    "                    raw_value = row.get(col)\n",
    "                    if (pd.notna(raw_value) and \n",
    "                        raw_value != '' and \n",
    "                        str(raw_value).lower() not in ['nat', 'none', 'null']):\n",
    "                        standardized_value = self.standardize_monetary(raw_value)\n",
    "                        if standardized_value is not None:\n",
    "                            final_value = standardized_value\n",
    "                            break\n",
    "            \n",
    "            result_df.at[idx, consolidated_column] = final_value\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def standardize_monetary(self, monetary_value) -> Optional[float]:\n",
    "        if pd.isna(monetary_value) or monetary_value is None or monetary_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            value_str = str(monetary_value).strip()\n",
    "            \n",
    "            if value_str.lower() in ['nat', 'none', 'null', '', 'not available', 'n/a']:\n",
    "                return None\n",
    "            \n",
    "            # Check if it's already a valid number\n",
    "            if isinstance(monetary_value, (int, float)) and not pd.isna(monetary_value):\n",
    "                if monetary_value < 0:\n",
    "                    return None\n",
    "                return round(float(monetary_value), 2)\n",
    "            \n",
    "            # Word version\n",
    "            if re.match(r'^[a-zA-Z\\s]+$', value_str):\n",
    "                converted_float = self.convert_text_to_number(value_str)\n",
    "                if converted_float is None:\n",
    "                    return None\n",
    "                if converted_float < 0:\n",
    "                    return None\n",
    "                return round(converted_float, 2)\n",
    "            \n",
    "            # Remove currency symbols and common formatting for numeric values\n",
    "            value_str = re.sub(r'[$£€¥₹]', '', value_str)\n",
    "            value_str = value_str.replace(',', '').replace(' ', '') \n",
    "            \n",
    "            if value_str == '':\n",
    "                return None\n",
    "            \n",
    "            multiplier = 1\n",
    "            value_str_lower = value_str.lower()\n",
    "            if value_str_lower.endswith('k'):\n",
    "                multiplier = 1000\n",
    "                value_str = value_str[:-1]\n",
    "            elif value_str_lower.endswith('m'):\n",
    "                multiplier = 1000000\n",
    "                value_str = value_str[:-1]\n",
    "            elif value_str_lower.endswith('b'):\n",
    "                multiplier = 1000000000\n",
    "                value_str = value_str[:-1]\n",
    "            \n",
    "            try:\n",
    "                final_float = float(value_str) * multiplier\n",
    "            except ValueError:\n",
    "                return None\n",
    "            \n",
    "            if final_float < 0:\n",
    "                return None\n",
    "            \n",
    "            return round(final_float, 2)\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    def convert_text_to_number(self, text_value: str) -> Optional[float]:\n",
    "        if not text_value:\n",
    "            return None\n",
    "            \n",
    "        text_value = text_value.lower().strip()\n",
    "        \n",
    "        number_words = {\n",
    "            'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,\n",
    "            'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10,\n",
    "            'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15,\n",
    "            'sixteen': 16, 'seventeen': 17, 'eighteen': 18, 'nineteen': 19, 'twenty': 20,\n",
    "            'thirty': 30, 'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70,\n",
    "            'eighty': 80, 'ninety': 90, 'hundred': 100, 'thousand': 1000, 'million': 1000000,\n",
    "            'billion': 1000000000\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            words = text_value.split()\n",
    "            total = 0\n",
    "            current = 0\n",
    "            found_valid_word = False\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.strip()\n",
    "                if word in number_words:\n",
    "                    found_valid_word = True\n",
    "                    value = number_words[word]\n",
    "                    if value == 100:\n",
    "                        current = current * 100 if current > 0 else 100\n",
    "                    elif value >= 1000:\n",
    "                        total += current * value\n",
    "                        current = 0\n",
    "                    else:\n",
    "                        current += value\n",
    "            \n",
    "            total += current\n",
    "            \n",
    "            if not found_valid_word or (total == 0 and 'zero' not in text_value):\n",
    "                return None\n",
    "            \n",
    "            return float(total)\n",
    "        \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def consolidate_last_activity(self) -> pd.DataFrame:\n",
    "        \n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_last_activity'] = None\n",
    "        \n",
    "        last_activity_columns = ['Last Activity']\n",
    "        \n",
    "        # Define placeholder values to filter out\n",
    "        placeholder_values = [\n",
    "            '42', 42, 'Called Client', 'called client', 'CALLED CLIENT', ''\n",
    "        ]\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_last_activity = None\n",
    "            \n",
    "            for col in last_activity_columns:\n",
    "                if col in result_df.columns:\n",
    "                    raw_activity = row.get(col)\n",
    "                    \n",
    "                    # Skip if NaN, empty, or placeholder value\n",
    "                    if (pd.isna(raw_activity) or \n",
    "                        raw_activity == '' or \n",
    "                        raw_activity in placeholder_values or\n",
    "                        str(raw_activity).strip().lower() in [str(p).lower() for p in placeholder_values]):\n",
    "                        continue\n",
    "                    \n",
    "                    # Try to format as date using existing method\n",
    "                    formatted_date = self.format_date_to_standard(raw_activity)\n",
    "                    if formatted_date:\n",
    "                        final_last_activity = formatted_date\n",
    "                        break\n",
    "                    \n",
    "            \n",
    "            result_df.at[idx, 'consolidated_last_activity'] = final_last_activity\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_custom_field(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_custom_field'] = None\n",
    "        \n",
    "        custom_field_columns = ['Custom Field']\n",
    "        \n",
    "        placeholder_values = ['N/A', '{\"type\": null}', None, '', 'null', 'nat', 'none']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_custom_field = None\n",
    "            \n",
    "            for col in custom_field_columns:\n",
    "                if col in result_df.columns:\n",
    "                    custom_field_val = row.get(col)\n",
    "                    \n",
    "                    if (pd.notna(custom_field_val) and \n",
    "                        custom_field_val != '' and \n",
    "                        custom_field_val not in placeholder_values and\n",
    "                        str(custom_field_val).lower() not in [str(p).lower() for p in placeholder_values if p is not None]):\n",
    "                        final_custom_field = custom_field_val\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_custom_field'] = final_custom_field\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def consolidate_region(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_region'] = None\n",
    "        \n",
    "        region_columns = ['Region']\n",
    "        \n",
    "        north_america_values = ['North America', 'NA', 'N.A.', 'United States', 'US']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_region = None\n",
    "            \n",
    "            for col in region_columns:\n",
    "                if col in result_df.columns:\n",
    "                    region_val = row.get(col)\n",
    "                    \n",
    "                    if (pd.notna(region_val) and \n",
    "                        region_val != '' and \n",
    "                        str(region_val).strip() in north_america_values):\n",
    "                        final_region = 'North America'\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_region'] = final_region\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_random_notes(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_random_notes'] = None\n",
    "        \n",
    "        random_notes_columns = ['Random Notes']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_notes_flag = None\n",
    "            \n",
    "            for col in random_notes_columns:\n",
    "                if col in result_df.columns:\n",
    "                    notes_val = row.get(col)\n",
    "                    \n",
    "                    # Check if value is \"See notes\" or \"Valid\"\n",
    "                    if (pd.notna(notes_val) and \n",
    "                        notes_val != '' and \n",
    "                        str(notes_val).strip() in ['See notes', 'Valid']):\n",
    "                        final_notes_flag = 'Notes_Flag'\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_random_notes'] = final_notes_flag\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "\n",
    "    def consolidate_deal_score(self) -> pd.DataFrame:\n",
    "\n",
    "        result_df = self.df.copy()\n",
    "        result_df['consolidated_deal_score'] = None\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            deal_score_val = row.get('Deal Score')\n",
    "            \n",
    "            if pd.notna(deal_score_val):\n",
    "                result_df.at[idx, 'consolidated_deal_score'] = deal_score_val / 100.0\n",
    "            else:\n",
    "                result_df.at[idx, 'consolidated_deal_score'] = None\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_engagement_level(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        result_df['consolidated_engagement_level'] = None\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            engagement_val = row.get('Engagement Level')\n",
    "            \n",
    "            if pd.notna(engagement_val):\n",
    "                result_df.at[idx, 'consolidated_engagement_level'] = round(engagement_val, 4)\n",
    "            else:\n",
    "                result_df.at[idx, 'consolidated_engagement_level'] = None\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    \n",
    "    def consolidate_num_calls(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        result_df['consolidated_num_calls'] = None\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            num_calls_val = row.get('Num Calls')\n",
    "            \n",
    "            if pd.notna(num_calls_val):\n",
    "                result_df.at[idx, 'consolidated_num_calls'] = num_calls_val\n",
    "            else:\n",
    "                result_df.at[idx, 'consolidated_num_calls'] = None\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_page_duration(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        result_df['consolidated_page_duration'] = None\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            time_val = row.get('Time on Page (sec)')\n",
    "            \n",
    "            if pd.notna(time_val):\n",
    "                result_df.at[idx, 'consolidated_page_duration'] = int(time_val)\n",
    "            else:\n",
    "                result_df.at[idx, 'consolidated_page_duration'] = None\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def consolidate_location(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_city'] = None\n",
    "        result_df['consolidated_state'] = None\n",
    "        result_df['consolidated_country'] = None\n",
    "        \n",
    "        state_city_mapping = {\n",
    "            'illinois': {'state': 'Illinois', 'city': 'Chicago'},\n",
    "            'il': {'state': 'Illinois', 'city': 'Chicago'},\n",
    "            'new york': {'state': 'New York', 'city': 'New York City'},\n",
    "            'n.y.': {'state': 'New York', 'city': 'New York City'},\n",
    "            'ny': {'state': 'New York', 'city': 'New York City'},\n",
    "            'california': {'state': 'California', 'city': 'San Francisco'},\n",
    "            'ca': {'state': 'California', 'city': 'San Francisco'}\n",
    "        }\n",
    "        \n",
    "        city_state_mapping = {\n",
    "            'chicago': {'state': 'Illinois', 'city': 'Chicago'},\n",
    "            'new york': {'state': 'New York', 'city': 'New York City'},\n",
    "            'nyc': {'state': 'New York', 'city': 'New York City'},\n",
    "            'san francisco': {'state': 'California', 'city': 'San Francisco'}\n",
    "        }\n",
    "        \n",
    "        us_variations = ['united states', 'us', 'usa', 'u.s.']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            state_val = row.get('State')\n",
    "            city_val = row.get('City')\n",
    "            country_val = row.get('Country')\n",
    "            \n",
    "            final_state = None\n",
    "            final_city = None\n",
    "            final_country = None\n",
    "            \n",
    "            # Priority 1: Use State to determine both state and city\n",
    "            if pd.notna(state_val) and state_val != '':\n",
    "                state_normalized = str(state_val).strip().lower()\n",
    "                if state_normalized in state_city_mapping:\n",
    "                    mapping = state_city_mapping[state_normalized]\n",
    "                    final_state = mapping['state']\n",
    "                    final_city = mapping['city']\n",
    "            \n",
    "            # Priority 2: If no valid state, use City to determine both city and state\n",
    "            if final_state is None and pd.notna(city_val) and city_val != '':\n",
    "                city_normalized = str(city_val).strip().lower()\n",
    "                if city_normalized in city_state_mapping:\n",
    "                    mapping = city_state_mapping[city_normalized]\n",
    "                    final_state = mapping['state']\n",
    "                    final_city = mapping['city']\n",
    "            \n",
    "            # Country logic: \n",
    "            # If state or city is filled, use \"United States\"\n",
    "            # If neither state nor city is filled, check if country was already filled\n",
    "            if final_state is not None or final_city is not None:\n",
    "                final_country = 'United States'\n",
    "            elif pd.notna(country_val) and country_val != '':\n",
    "                country_normalized = str(country_val).strip().lower()\n",
    "                if country_normalized in us_variations:\n",
    "                    final_country = 'United States'\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_state'] = final_state\n",
    "            result_df.at[idx, 'consolidated_city'] = final_city\n",
    "            result_df.at[idx, 'consolidated_country'] = final_country\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def create_clean_dataset(self) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        consolidated_df = self.consolidate_account_and_email()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_created_date()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_lead_source()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_monetary('opportunity_amount')\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_is_active()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_sfdc_id()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_monetary('annual_revenue')\n",
    "\n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_last_activity()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_custom_field()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_region()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "    \n",
    "        consolidated_df = self.consolidate_random_notes()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "\n",
    "        # Create intermediate cleaned dataset up to notes_flag\n",
    "        intermediate_df = consolidated_df.copy()\n",
    "        \n",
    "        intermediate_df['account_name'] = consolidated_df['consolidated_account_name']\n",
    "        intermediate_df['contact_email'] = consolidated_df['consolidated_contact_email']\n",
    "        intermediate_df['created_date'] = consolidated_df['consolidated_created_date']\n",
    "        intermediate_df['lead_source'] = consolidated_df['consolidated_lead_source']\n",
    "        intermediate_df['opportunity_amount'] = consolidated_df['consolidated_opportunity_amount']\n",
    "        intermediate_df['is_active'] = consolidated_df['consolidated_is_active']\n",
    "        intermediate_df['sfdc_id'] = consolidated_df['consolidated_sfdc_id']\n",
    "        intermediate_df['annual_revenue'] = consolidated_df['consolidated_annual_revenue']\n",
    "        intermediate_df['last_activity'] = consolidated_df['consolidated_last_activity']\n",
    "        intermediate_df['custom_field'] = consolidated_df['consolidated_custom_field']\n",
    "        intermediate_df['region'] = consolidated_df['consolidated_region']\n",
    "        intermediate_df['notes_flag'] = consolidated_df['consolidated_random_notes']\n",
    "        \n",
    "        # Drop columns for intermediate dataset\n",
    "        intermediate_columns_to_drop = [\n",
    "            'consolidated_account_name', 'consolidated_contact_email', 'consolidated_created_date',\n",
    "            'consolidated_lead_source', 'consolidated_opportunity_amount', 'consolidated_is_active',\n",
    "            'consolidated_sfdc_id', 'consolidated_last_activity', 'consolidated_annual_revenue',\n",
    "            'consolidated_custom_field', 'consolidated_region', 'consolidated_random_notes',\n",
    "            'Account Name', 'AccountName', 'Contact Email', 'Created Date', 'Lead Source', 'Time on Page (sec)',\n",
    "            'Opportunity Amount', 'Is Active', 'SFDC ID', 'Last Activity', 'Annual Revenue',\n",
    "            'Custom Field', 'Region', 'Random Notes', 'City', 'State', 'Country', 'Unnamed: 0', 'Unnamed: 21'  \n",
    "        ]\n",
    "        \n",
    "        intermediate_columns_to_drop = [col for col in intermediate_columns_to_drop if col in intermediate_df.columns]\n",
    "        intermediate_df = intermediate_df.drop(columns=intermediate_columns_to_drop)\n",
    "\n",
    "        consolidated_df = self.consolidate_deal_score()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "\n",
    "        consolidated_df = self.consolidate_engagement_level()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "\n",
    "        consolidated_df = self.consolidate_num_calls()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "\n",
    "        consolidated_df = self.consolidate_page_duration()\n",
    "\n",
    "        self.df = consolidated_df\n",
    "    \n",
    "        consolidated_df = self.consolidate_location()\n",
    "        \n",
    "        clean_df = consolidated_df.copy()\n",
    "        \n",
    "        clean_df['account_name'] = consolidated_df['consolidated_account_name']\n",
    "        clean_df['contact_email'] = consolidated_df['consolidated_contact_email']\n",
    "        clean_df['created_date'] = consolidated_df['consolidated_created_date']\n",
    "        clean_df['lead_source'] = consolidated_df['consolidated_lead_source']\n",
    "        clean_df['opportunity_amount'] = consolidated_df['consolidated_opportunity_amount']\n",
    "        clean_df['is_active'] = consolidated_df['consolidated_is_active']\n",
    "        clean_df['sfdc_id'] = consolidated_df['consolidated_sfdc_id']\n",
    "        clean_df['annual_revenue'] = consolidated_df['consolidated_annual_revenue']\n",
    "        clean_df['last_activity'] = consolidated_df['consolidated_last_activity']\n",
    "        clean_df['custom_field'] = consolidated_df['consolidated_custom_field']\n",
    "        clean_df['region'] = consolidated_df['consolidated_region']\n",
    "        clean_df['notes_flag'] = consolidated_df['consolidated_random_notes']\n",
    "        clean_df['deal_score'] = consolidated_df['consolidated_deal_score']\n",
    "        clean_df['engagement_level'] = consolidated_df['consolidated_engagement_level']\n",
    "        clean_df['num_calls'] = consolidated_df['consolidated_num_calls']\n",
    "        clean_df['page_duration'] = consolidated_df['consolidated_page_duration']\n",
    "        clean_df['city'] = consolidated_df['consolidated_city']\n",
    "        clean_df['state'] = consolidated_df['consolidated_state']\n",
    "        clean_df['country'] = consolidated_df['consolidated_country']\n",
    "            \n",
    "        columns_to_drop = [\n",
    "            'consolidated_account_name', 'consolidated_contact_email', 'consolidated_created_date',\n",
    "            'consolidated_lead_source', 'consolidated_opportunity_amount', 'consolidated_is_active', 'consolidated_engagement_level',\n",
    "            'consolidated_sfdc_id', 'consolidated_last_activity', 'consolidated_annual_revenue', 'consolidated_deal_score',\n",
    "            'consolidated_custom_field', 'consolidated_region', 'consolidated_random_notes', 'consolidated_num_calls', 'consolidated_page_duration',\n",
    "            'consolidated_city', 'consolidated_state', 'consolidated_country',\n",
    "            'Account Name', 'AccountName', 'Contact Email', 'Created Date', 'Lead Source', 'Time on Page (sec)',\n",
    "            'Opportunity Amount', 'Is Active', 'SFDC ID', 'Last Activity', 'Annual Revenue', 'Deal Score', 'Engagement Level', 'Num Calls',\n",
    "            'Custom Field', 'Region', 'Random Notes', 'City', 'State', 'Country', 'Unnamed: 0', 'Unnamed: 21'  \n",
    "        ]\n",
    "        \n",
    "        columns_to_drop = [col for col in columns_to_drop if col in clean_df.columns]\n",
    "        clean_df = clean_df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        return intermediate_df, clean_df\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Info:\n",
      "Shape: (500, 30)\n",
      "Columns: ['Account Name', 'account_name', 'AccountName', 'Contact Email', 'contact_email', 'Created Date', 'created_date', 'Lead Source', 'lead_source', 'Opportunity Amount', 'opportunity_amount', 'Is Active', 'is_active', 'SFDC ID', 'sfdc_id', 'Annual Revenue', 'annual_revenue', 'Last Activity', 'Custom Field', 'Region', 'Unnamed: 0', 'Unnamed: 21', 'Random Notes', 'Deal Score', 'Engagement Level', 'Num Calls', 'Time on Page (sec)', 'City', 'State', 'Country']\n",
      "\n",
      "Intermediate dataset saved as 'IntermediateSalesforceData.csv'\n",
      "\n",
      "Full clean dataset saved as 'CleanSalesforceData.csv'\n",
      "Full clean dataset shape: (500, 19)\n",
      "\n",
      "Sample of full cleaned data:\n",
      "  account_name     contact_email created_date     lead_source  \\\n",
      "0    Acme Corp  contact@acme.com   2020-01-01      Trade Show   \n",
      "1       Globex   help@globex.com   2022-12-01    Social Media   \n",
      "2       Globex   help@globex.com         None  Email Campaign   \n",
      "3    Acme Corp  contact@acme.com         None    Social Media   \n",
      "4     Umbrella              None   2024-03-26             Web   \n",
      "5       Globex   help@globex.com   2024-04-03            None   \n",
      "6      Soylent              None   2025-07-17        Referral   \n",
      "7      Soylent              None   2023-12-17   Phone Inquiry   \n",
      "8    Acme Corp  contact@acme.com   2022-12-01      Trade Show   \n",
      "9    Acme Corp  contact@acme.com         None  Email Campaign   \n",
      "\n",
      "  opportunity_amount is_active     sfdc_id annual_revenue last_activity  \\\n",
      "0             1000.0     False  001B000002      1000000.0          None   \n",
      "1               None     False        None      1000000.0          None   \n",
      "2            50000.0      True  001A000001      5000000.0    2024-05-05   \n",
      "3            50000.0     False        None      1000000.0    2024-09-13   \n",
      "4            50000.0     False  001B000002      1000000.0    2024-05-05   \n",
      "5            50000.0      True  001A000001       999999.0          None   \n",
      "6            10000.0     False  001A000001           None    2025-01-28   \n",
      "7             1000.0      True        None           None          None   \n",
      "8             1000.0     False  001B000002      2500000.0    2025-01-23   \n",
      "9            50000.0     False  001C000003      5000000.0          None   \n",
      "\n",
      "                  custom_field         region  notes_flag deal_score  \\\n",
      "0  {\"type\": \"A\", \"value\": 100}  North America        None        0.5   \n",
      "1                         None  North America        None       None   \n",
      "2                         None  North America        None       None   \n",
      "3  {\"type\": \"A\", \"value\": 100}  North America        None        1.0   \n",
      "4                         None  North America        None        1.0   \n",
      "5  {\"type\": \"A\", \"value\": 100}  North America        None       None   \n",
      "6  {\"type\": \"B\", \"value\": 200}  North America        None        1.0   \n",
      "7  {\"type\": \"A\", \"value\": 100}  North America  Notes_Flag        0.1   \n",
      "8                         None           None        None        0.5   \n",
      "9                         None  North America  Notes_Flag       None   \n",
      "\n",
      "  engagement_level num_calls page_duration           city       state  \\\n",
      "0             None      None           141  New York City    New York   \n",
      "1           0.0636      31.0           278  San Francisco  California   \n",
      "2           0.2173      38.0           271        Chicago    Illinois   \n",
      "3             None      32.0           318           None        None   \n",
      "4           0.5376      None           399  New York City    New York   \n",
      "5           0.7661      23.0           383        Chicago    Illinois   \n",
      "6             None       3.0           155        Chicago    Illinois   \n",
      "7           0.9034      37.0           248        Chicago    Illinois   \n",
      "8           0.5414      None           419        Chicago    Illinois   \n",
      "9             None      10.0           108  New York City    New York   \n",
      "\n",
      "         country  \n",
      "0  United States  \n",
      "1  United States  \n",
      "2  United States  \n",
      "3  United States  \n",
      "4  United States  \n",
      "5  United States  \n",
      "6  United States  \n",
      "7  United States  \n",
      "8  United States  \n",
      "9  United States  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/DirtySalesforceData.csv')\n",
    "\n",
    "print(\"Original Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "cleaner = Cleansing(df)\n",
    "\n",
    "# Create both datasets - returns a tuple (intermediate_df, full_clean_df)\n",
    "intermediate_df, clean_df = cleaner.create_clean_dataset()\n",
    "\n",
    "# Save intermediate dataset (cleaned up to notes_flag)\n",
    "intermediate_df.to_csv('IntermediateSalesforceData.csv', index=False)\n",
    "print(f\"\\nIntermediate dataset saved as 'IntermediateSalesforceData.csv'\")\n",
    "\n",
    "# Save full cleaned dataset\n",
    "clean_df.to_csv('CleanSalesforceData.csv', index=False)\n",
    "print(f\"\\nFull clean dataset saved as 'CleanSalesforceData.csv'\")\n",
    "print(f\"Full clean dataset shape: {clean_df.shape}\")\n",
    "\n",
    "print(f\"\\nSample of full cleaned data:\")\n",
    "print(clean_df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written Responses\n",
    "\n",
    "## Explain your assumptions where data is ambiguous or incorrect:\n",
    "\n",
    "### Ambiguous Data Assumptions\n",
    "\n",
    "One thing that caught my eye when I first read the data was the amount of duplicate columns. I knew that eye balling would not be sufficient enough in deciding which columns to keep. I decided to use a python script to help me with this task. This is found in the duplicate-column-insight.ipynb file. Here I creted a ColumnQuality class that had four tests that would give a score for each column on different metrics. The four tests were:\n",
    "\n",
    "1. Completeness - This would give me a percentage of how much column was filled with data. The higher the better.\n",
    "2. Quality - This would give me a percentage of how much of a columns had valid data. I would consider invalid data to be anything with one of these values - 'not_a_date', 'noemail', 'invalid@', 'BAD_ID', 'N/A', 'NaT', 'ERROR'. The higher the better.\n",
    "3. Format Consistency - This would give me a percentage of how much of a columns had consistent data. This could only apply to columns with a date, boolean or email. The higher the better.\n",
    "4. Data Type - This would give me a percentage of how much of a columns had consistent data. This could only apply to columns with values that were numeric or strings. The higher the better.\n",
    "\n",
    "I then compared these scores for each columns and decided to prioritize using data from the columns with the best score. \n",
    "\n",
    "I also found the Random Notes column to be quite ambigous. So I ran a pythons cript in the same file to see what percentage each of the values occurred. Analyzing if one value stood out allowed me to discern the significance of the column and its meaning. Unfortunately I could not discern any meaning from the column, but figure it would be bad practice to completely remove it. I will get into what I did with this column later. \n",
    "\n",
    "I also found the deal score column to be amigous. I was worried that this column could be correlated to the Opportunity Amount columns and the Annual Revenue columns. So I created a script that would find the correlation between these columns and the deal score column. I found that the correlation was not significant enough to justify using a different strategy when cleaning this column later on. \n",
    "\n",
    "### Incorrect Data Assumptions\n",
    "\n",
    "Making assumpitons for incorrect data was much easier as I could clearly see which data was just wrong and not valid record value. For instance Tthere were various malformed email addresses that had values with no domain, a missing @ symbol, or jst containing the value no email. Recognizing these things allowed me to determine which email addresesses were wellformed and which were not. \n",
    "\n",
    "I also saw invalid date formats that were clearly inconsistent or contained values like NaT and not_a_date. I made the conclusion that the best formed date values would be in the format YYYY-MM-DD as this is commonly used in salesforce systems, therfore anything else would have to be converted.\n",
    "\n",
    "Overall accross these columns I saw malformed data of all sorts. Numeric columns often mixed strings and numbers. Boolean columns used strings of binary values. Locational columns often mixed Abbreviations, full names, and even the casing varied. Across all of these place holder values that were not valid records even varied within the same column. To resolve these issues I defualted to the way data like this would be represented in salesforce systems. \n",
    "\n",
    "## Identify and describe gaps or potential data quality risks:\n",
    "    \n",
    "Like I menitoned the duplicate columns flagged a potential risk. This suggested to me that there were integration issues where multiple stytems were not properly consolidated. Like I mentioned I used a script to help me with this task. Something else that was alarming was the amount of missing data within the columns that told me the data was not being collected consistently or properly from the source. \n",
    "\n",
    "There wer also strucutral gaps like an unamed column and one that just contained row indexes. This suggested that the software or scripting used for collection was not consistent or formattically sound. The columns themselves were also inconsistent with naming. The mixed CamelCase, snake_case, and just spaces. Within the columns there was also mixing of data types, where at times the same column would contain strings and numbers. \n",
    "\n",
    "Additionally like I mentioned before I found that the valus of the data itself was was inonsistent. For instance the Boolean columns contaiend various ways of reprpesenting true and false values. Additioanlly string values mixed different reprsenations of the values where at times full names were used and at other times abbreviations. I also found that data simply did not make sense. An example would be a record that had a state value of Illinois but a city value of New York City. These gaps highlighted the disregard for data quality and consistency. \n",
    "\n",
    "\n",
    "## Justify each of transformation or cleaning step in your code:\n",
    "\n",
    "1. Duplicate Column Consolidation Strategy\n",
    "\n",
    "Implementation: Created a duplicate_groups dictionary to map final fields to their various column representations.\n",
    "\n",
    "Justification: Rather than arbitrarily choosing columns, I used my ColumnQualityAnalyzer to score each duplicate column on completeness, quality, format consistency, and data type consistency. This approach allowed me to identify the most reliable columns to use for each final field, and create a prioritization order. Anytime a column was not found in the data I would use the next best column based on the priority order that was determined by the ColumnQualityAnalyzer. \n",
    "\n",
    "2. Account Name and Contact Email Consolidation\n",
    "\n",
    "Implementation: Consolidated account names and emails together, with logic to derive missing information from valid counterparts.\n",
    "\n",
    "Justification: These fields are almost always consistent with each other in CRM Systems.. When an account name was missing but a valid email existed (help@globex.com or contact@acme.com), I could reliably infer the account name. Conversely, when account names existed without valid emails, I generated the corresponding corporate emails. I would always priortiize to infer based on the account name as its scores were higher than the email fields. I would only infer in the other direction if all of the account names were missing. Laslty many of the account names did not have a valid email so we simply used None. \n",
    "\n",
    "3. Date Standardization \n",
    "\n",
    "Implementation: Standardized all dates to YYYY-MM-DD format using pandas to_datetime.\n",
    "\n",
    "I chose ISO 8601 format (YYYY-MM-DD) as it's universally recognized, sorts correctly, and is commonly used in Salesforce systems. Again anything that was not a valid date was set to None.\n",
    "\n",
    "4. Lead Source Consolidation\n",
    "\n",
    "Implementation: Consolidated lead sources to a single field using the Duplicate Column Strategy.\n",
    "\n",
    "Justification: This was a catergory where any valid value is prefered to null. Still I prioritized lead_source since it was the highest scoring field. \n",
    "\n",
    "5. Is Active Consolidation\n",
    "\n",
    "Implementation: Mapped various string representations to proper boolean values.\n",
    "\n",
    "Justification: Here I simply mapped the various represenations of boolean values to a standardized boolean value of True or False. \n",
    "\n",
    "6. SFDC ID Consolidation\n",
    "\n",
    "Implementation: Filtered out obvious placeholder values and kept legitimate IDs. I did this again following the Duplicate Column Strategy.\n",
    "\n",
    "Justification: Placeholder patterns like ('abc123', 'xyz-00001', '12345', 'bad_id') clearly were not valid SFDC IDs and were filtered out. This would prevent false linking in systems. I decided between the columns which value to prioritize based on the scores from the Duplicate Column Strategy.\n",
    "\n",
    "7. Monetary Value Standardization\n",
    "\n",
    "Implementation: I parsed any monetary values by:\n",
    "    - Decided between the columns which value to prioritize based on the scores from the Duplicate Column Strategy\n",
    "    - Removing the currencey symbol ($)\n",
    "    - Converting text to number (five million -> 5000000.0) through Dictionary-based parsing of written numbers\n",
    "    - Handling Suffixes like K, M, and B\n",
    "\n",
    "Justification: Converting these values to a standard number format would allow easier comparison and analysis with this data down the line.\n",
    "\n",
    "9. Last Activity Consolidation\n",
    "\n",
    "Implementation: Here I filtered out placeholder values and kept legitimate dates. I again used the Duplicate Column Strategy to determine which column to prioritize.\n",
    "\n",
    "Justification: I decided to use date values and filter anuthing else out as other possible values such as 'Called Client' were not varied enough to provide meaning to this column. After filtering I standardized the dates.\n",
    "\n",
    "10. Preserving Custom Field\n",
    "\n",
    "Implementation: Here I kept any JSON values that were properly formatted. I got rid of any nulls or JSON values that represented nulls.\n",
    "\n",
    "Justification: Custom fields can contain data that is later on parsed seperatley and therefore I wanted to preserve values that were valid. I opted not to parse the valid JSON values as there was not enough context to try and give this data new meaning. \n",
    "\n",
    "11. Region Standardization\n",
    "\n",
    "    Implementation: Mapped various North America representations to a single standard value.\n",
    "\n",
    "    Justification: To keep consistencey standardized all variations ('North America', 'NA', 'N.A.', 'United States', 'US') to 'North America' as the most descriptive option.\n",
    "\n",
    "12. Location Data Standardization\n",
    "\n",
    "Implementation: I used the ColumnQualityAnalyzer to score State and City and to determine which column was basis for the ones that were related. State scored better so if a State and City were conflicting geographically I would populate the city based on the state. If either column was missing I would use the other column to infer the missing value. I also ensured format consistencey by making sure that cities, states and countries were all fully spelled out. \n",
    "\n",
    "Justification: I needed to enusre the the locational data was not only consistent between their respective columns, but also made sense across the records.\n",
    "\n",
    "13. Random Notes Flag\n",
    "\n",
    "Implementation: Converted the values 'See notes' and 'Valid' to a standard 'Notes_Flag'\n",
    "\n",
    "Justification: Since the values within the random notes column were not consistent and ambigous I tried to maintain information by marking these records in a standardized way.\n",
    "\n",
    "14. Deal Score Normaliation\n",
    "\n",
    "Implementation: Since the values ranged between 0 and 100 I decided to standardize the values to be between 0 and 1. \n",
    "\n",
    "Justification: I used scores elswhere in my code, and decimals were also used for the engagement level, so I chose to do the same here. \n",
    "\n",
    "15. Engagement Level Normalization\n",
    "\n",
    "Implementation: I rounded the values in this column to 4 decimal places.\n",
    "\n",
    "Justification: I wanted to preserve the accuracy of these values, but thought there need to be a standard to which the exactness of the data was measured. \n",
    "\n",
    "16. General Blank Values and Useless Columns\n",
    "\n",
    "Implementation: I dropped all columns that were not needed for the analysis, and filled all blank values across columns with None. Certain columns only needed there blank values to be filled with None and no further cleaning was needed.\n",
    "\n",
    "Justification: Columns deemed useless were dropped as they provided no apparent valuable information. As far as populating blanks with None, I wanted to make sure that even records without a value for a given column could be recognized as such.\n",
    "\n",
    "17. Column Naming\n",
    "\n",
    "Implementation: All output columns used snake_case naming.\n",
    "\n",
    "Justification: This would help with readability and also usage of the dataset down the line with things such as API integrations. \n",
    "\n",
    "\n",
    "# Moving Forward\n",
    "\n",
    "## Root Cause and Prevention\n",
    "\n",
    "I would start by tracking bad data back to the source. We should look at which system created this data and what changes were made to it along the way. I would also see how data changed over time and what patterns emerged. Next I would compare the error rates of data from different sources. For example we could analyze if manual entry was more prone to errors than automated entry. Finally I would look for patters in the errors to see if users, time periods, or processes kept producing bad data. This could tell us if it is a system problem or just not great training. \n",
    "\n",
    "## People Systems and Processes to Review\n",
    "\n",
    "The Salesforce team should check how the they set up field mappins and data rules. We could then see if they are accumulating data properly. We also need to review if data or IT teams are properly transferring data. We should check on their system connections and any programs that alter data between systems. The Sales team could be intervies to see how they actually enter data. We could also take a look at their training and test them to see if they are following proper procedures. \n",
    "\n",
    "## Data Producers Recommendations\n",
    "\n",
    "We should setup basic data checks that stop bad data from being submitted. THis could be making sure that emails include an @ sign, IDs follow the correct format, etc. While I am not to knowledgable of the logistics of this, I would suggest setting up some sort of automatic monriotring to show data quality problems. Lastly, setting up better training that is simplified and easily understandable, especially for those that submitting data. Settuping clear rules and defining processes would make it easier for users to follow proper procedures.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
