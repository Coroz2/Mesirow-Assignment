{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleansing:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        self.duplicate_groups = {\n",
    "            'account_name': ['Account Name', 'account_name', 'AccountName'],\n",
    "            'contact_email': ['Contact Email', 'contact_email'],\n",
    "            'created_date': ['Created Date', 'created_date'],\n",
    "            'lead_source': ['Lead Source', 'lead_source'],\n",
    "            'opportunity_amount': ['Opportunity Amount', 'opportunity_amount'],\n",
    "            'is_active': ['Is Active', 'is_active'],\n",
    "            'sfdc_id': ['SFDC ID', 'sfdc_id'],\n",
    "            'annual_revenue': ['Annual Revenue', 'annual_revenue']\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def is_valid_email(self, email: str) -> bool:\n",
    "        # Check if email is one of the valid emails\n",
    "        if pd.isna(email) or email is None:\n",
    "            return False\n",
    "        return email in ['help@globex.com', 'contact@acme.com']\n",
    "    \n",
    "    def is_placeholder_email(self, email: str) -> bool:\n",
    "        # Check if email is a placeholder \n",
    "        if pd.isna(email) or email is None or email == '':\n",
    "            return True\n",
    "        placeholders = ['noemail', 'invalid@', 'user@', 'missing.com', 'placeholder']\n",
    "        return any(placeholder in str(email).lower() for placeholder in placeholders)\n",
    "    \n",
    "    def get_corresponding_account(self, email: str) -> Optional[str]:\n",
    "        # Get the corresponding account name for a valid email\n",
    "        if email == 'help@globex.com':\n",
    "            return 'Globex'\n",
    "        elif email == 'contact@acme.com':\n",
    "            return 'Acme Corp'\n",
    "        return None\n",
    "    \n",
    "    def generate_email_for_account(self, account_name: str) -> Optional[str]:\n",
    "        # Generate corresponding email for Globex or Acme Corp\n",
    "        if account_name == 'Globex':\n",
    "            return 'help@globex.com'\n",
    "        elif account_name == 'Acme Corp':\n",
    "            return 'contact@acme.com'\n",
    "        return None\n",
    "    \n",
    "    def consolidate_account_and_email(self) -> pd.DataFrame:\n",
    "\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_account_name'] = None\n",
    "        result_df['consolidated_contact_email'] = None\n",
    "        \n",
    "        account_columns = ['account_name', 'AccountName', 'Account Name']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "\n",
    "            contact_email_lower_val = row.get('contact_email')\n",
    "            contact_email_val = row.get('Contact Email')\n",
    "            \n",
    "            contact_email_lower_valid = self.is_valid_email(contact_email_lower_val)\n",
    "            contact_email_valid = self.is_valid_email(contact_email_val)\n",
    "            \n",
    "            account_values = {}\n",
    "            for col in account_columns:\n",
    "                if col in result_df.columns:\n",
    "                    account_values[col] = row.get(col)\n",
    "            \n",
    "            final_account = None\n",
    "            final_email = None\n",
    "            \n",
    "            for col in account_columns:\n",
    "                if (col in account_values and \n",
    "                    pd.notna(account_values[col]) and \n",
    "                    account_values[col] != ''):\n",
    "                    final_account = account_values[col]\n",
    "                    break\n",
    "            \n",
    "            # If no account found, derive from valid email (all accounts empty case)\n",
    "            if final_account is None:\n",
    "                # Check contact_email first (priority)\n",
    "                if contact_email_lower_valid:\n",
    "                    final_account = self.get_corresponding_account(contact_email_lower_val)\n",
    "                    final_email = contact_email_lower_val\n",
    "                # If contact_email not valid, check Contact Email\n",
    "                elif contact_email_valid:\n",
    "                    final_account = self.get_corresponding_account(contact_email_val)\n",
    "                    final_email = contact_email_val\n",
    "            \n",
    "            if final_account is not None and final_email is None:\n",
    "                email_matched = False\n",
    "                \n",
    "                if contact_email_lower_valid:\n",
    "                    expected_account = self.get_corresponding_account(contact_email_lower_val)\n",
    "                    if final_account == expected_account:\n",
    "                        final_email = contact_email_lower_val\n",
    "                        email_matched = True\n",
    "                \n",
    "                if not email_matched and contact_email_valid:\n",
    "                    expected_account = self.get_corresponding_account(contact_email_val)\n",
    "                    if final_account == expected_account:\n",
    "                        final_email = contact_email_val\n",
    "                        email_matched = True\n",
    "                \n",
    "                # If no email match but account is Globex or Acme Corp, generate email\n",
    "                if not email_matched:\n",
    "                    generated_email = self.generate_email_for_account(final_account)\n",
    "                    if generated_email:\n",
    "                        final_email = generated_email\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_account_name'] = final_account\n",
    "            result_df.at[idx, 'consolidated_contact_email'] = final_email\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def consolidate_created_date(self) -> pd.DataFrame:\n",
    "    \n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_created_date'] = None\n",
    "        \n",
    "        created_date_columns = ['created_date', 'Created Date']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_created_date = None\n",
    "            \n",
    "            for col in created_date_columns:\n",
    "                if col in result_df.columns:\n",
    "                    raw_date = row.get(col)\n",
    "                    if (pd.isna(raw_date) or raw_date == '' or \n",
    "                        str(raw_date).lower() in ['nat', 'not_a_date', 'none']):\n",
    "                        continue\n",
    "                    \n",
    "                    formatted_date = self.format_date_to_standard(raw_date)\n",
    "                    if formatted_date:\n",
    "                        final_created_date = formatted_date\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_created_date'] = final_created_date\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def format_date_to_standard(self, date_value) -> Optional[str]:\n",
    "        if pd.isna(date_value) or date_value is None or date_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            parsed_date = pd.to_datetime(date_value, errors='coerce')\n",
    "            \n",
    "            if pd.isna(parsed_date):\n",
    "                return None\n",
    "            \n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    def consolidate_lead_source(self) -> pd.DataFrame:\n",
    "        result_df = self.df.copy()\n",
    "        \n",
    "        result_df['consolidated_lead_source'] = None\n",
    "        \n",
    "        lead_source_columns = ['lead_source', 'Lead Source']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            final_lead_source = None\n",
    "            \n",
    "            for col in lead_source_columns:\n",
    "                if col in result_df.columns:\n",
    "                    lead_source_val = row.get(col)\n",
    "                    if (pd.notna(lead_source_val) and \n",
    "                        lead_source_val != '' and \n",
    "                        str(lead_source_val).lower() not in ['nat', 'none', 'null']):\n",
    "                        final_lead_source = lead_source_val\n",
    "                        break\n",
    "            \n",
    "            result_df.at[idx, 'consolidated_lead_source'] = final_lead_source\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    def create_clean_dataset(self) -> pd.DataFrame:\n",
    "        consolidated_df = self.consolidate_account_and_email()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_created_date()\n",
    "        \n",
    "        self.df = consolidated_df\n",
    "        \n",
    "        consolidated_df = self.consolidate_lead_source()\n",
    "        \n",
    "        clean_df = consolidated_df.copy()\n",
    "        \n",
    "        clean_df['account_name'] = consolidated_df['consolidated_account_name']\n",
    "        clean_df['contact_email'] = consolidated_df['consolidated_contact_email']\n",
    "        clean_df['created_date'] = consolidated_df['consolidated_created_date']\n",
    "        clean_df['lead_source'] = consolidated_df['consolidated_lead_source']\n",
    "        \n",
    "        columns_to_drop = [\n",
    "            'consolidated_account_name', 'consolidated_contact_email', 'consolidated_created_date',\n",
    "            'consolidated_lead_source', 'Account Name', 'AccountName', 'Contact Email', \n",
    "            'Created Date', 'Lead Source'\n",
    "        ]\n",
    "        \n",
    "        columns_to_drop = [col for col in columns_to_drop if col in clean_df.columns]\n",
    "        clean_df = clean_df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        return clean_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing format_date_to_standard:\n",
      "'2023-01-15': 2023-01-15 (should be 2023-01-15)\n",
      "'01/15/2023': 2023-01-15 (should be 2023-01-15)\n",
      "'15-01-2023': 2023-01-15 (should be 2023-01-15)\n",
      "'2023/01/15': 2023-01-15 (should be 2023-01-15)\n",
      "'Jan 15, 2023': 2023-01-15 (should be 2023-01-15)\n",
      "'15 January 2023': 2023-01-15 (should be 2023-01-15)\n",
      "'2023-1-5': 2023-01-05 (should be 2023-01-05)\n",
      "None: None (should be None)\n",
      "empty string: None (should be None)\n",
      "'invalid_date': None (should be None)\n",
      "'2023-13-45': None (should be None - invalid date)\n",
      "'not a date': None (should be None)\n",
      "\n",
      "Testing is_valid_date_format:\n",
      "'2023-01-15': True (should be True)\n",
      "'2023-12-31': True (should be True)\n",
      "'2023-1-5': False (should be False - wrong format)\n",
      "'01/15/2023': False (should be False - wrong format)\n",
      "'2023/01/15': False (should be False - wrong format)\n",
      "'Jan 15, 2023': False (should be False - wrong format)\n",
      "'2023-13-45': False (should be False - invalid date)\n",
      "None: False (should be False)\n",
      "empty string: False (should be False)\n",
      "'not a date': False (should be False)\n",
      "'2023-02-29': False (should be False - not a leap year)\n",
      "'2024-02-29': True (should be True - leap year)\n",
      "\n",
      "Testing edge cases:\n",
      "'2023-01-01': 2023-01-01 (should be 2023-01-01)\n",
      "'2023-12-31': 2023-12-31 (should be 2023-12-31)\n",
      "'1900-01-01': 1900-01-01 (should be 1900-01-01)\n",
      "'2100-12-31': 2100-12-31 (should be 2100-12-31)\n",
      "'02/29/2024': 2024-02-29 (should be 2024-02-29 - leap year)\n",
      "'02/29/2023': None (should be None - not leap year)\n",
      "123456789: 1970-01-01 (should handle numeric input)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/mgvcrgd90878wk8lmjdc_4pc0000gn/T/ipykernel_2237/3549134330.py:15: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  parsed_date = pd.to_datetime(date_value, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "class DateTester:\n",
    "    def format_date_to_standard(self, date_value) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Format date to standard YYYY-MM-DD format\n",
    "        Handles various input formats and returns None for invalid dates\n",
    "        \"\"\"\n",
    "        if pd.isna(date_value) or date_value is None or date_value == '':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Convert to pandas datetime which handles many formats automatically\n",
    "            parsed_date = pd.to_datetime(date_value, errors='coerce')\n",
    "            \n",
    "            # If parsing failed, return None\n",
    "            if pd.isna(parsed_date):\n",
    "                return None\n",
    "            \n",
    "            # Return in standard YYYY-MM-DD format\n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    def is_valid_date_format(self, date_string) -> bool:\n",
    "        \"\"\"\n",
    "        Check if date string is in the standard YYYY-MM-DD format\n",
    "        \"\"\"\n",
    "        if pd.isna(date_string) or date_string is None or date_string == '':\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Check if it matches YYYY-MM-DD pattern exactly\n",
    "            if len(str(date_string)) == 10 and str(date_string).count('-') == 2:\n",
    "                parts = str(date_string).split('-')\n",
    "                if (len(parts[0]) == 4 and len(parts[1]) == 2 and len(parts[2]) == 2 and\n",
    "                    parts[0].isdigit() and parts[1].isdigit() and parts[2].isdigit()):\n",
    "                    # Also verify it's a valid date\n",
    "                    pd.to_datetime(date_string, format='%Y-%m-%d', errors='raise')\n",
    "                    return True\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "dt = DateTester()\n",
    "\n",
    "# Test format_date_to_standard\n",
    "print(\"Testing format_date_to_standard:\")\n",
    "print(f\"'2023-01-15': {dt.format_date_to_standard('2023-01-15')} (should be 2023-01-15)\")\n",
    "print(f\"'01/15/2023': {dt.format_date_to_standard('01/15/2023')} (should be 2023-01-15)\")\n",
    "print(f\"'15-01-2023': {dt.format_date_to_standard('15-01-2023')} (should be 2023-01-15)\")\n",
    "print(f\"'2023/01/15': {dt.format_date_to_standard('2023/01/15')} (should be 2023-01-15)\")\n",
    "print(f\"'Jan 15, 2023': {dt.format_date_to_standard('Jan 15, 2023')} (should be 2023-01-15)\")\n",
    "print(f\"'15 January 2023': {dt.format_date_to_standard('15 January 2023')} (should be 2023-01-15)\")\n",
    "print(f\"'2023-1-5': {dt.format_date_to_standard('2023-1-5')} (should be 2023-01-05)\")\n",
    "print(f\"None: {dt.format_date_to_standard(None)} (should be None)\")\n",
    "print(f\"empty string: {dt.format_date_to_standard('')} (should be None)\")\n",
    "print(f\"'invalid_date': {dt.format_date_to_standard('invalid_date')} (should be None)\")\n",
    "print(f\"'2023-13-45': {dt.format_date_to_standard('2023-13-45')} (should be None - invalid date)\")\n",
    "print(f\"'not a date': {dt.format_date_to_standard('not a date')} (should be None)\")\n",
    "print()\n",
    "\n",
    "# Test is_valid_date_format\n",
    "print(\"Testing is_valid_date_format:\")\n",
    "print(f\"'2023-01-15': {dt.is_valid_date_format('2023-01-15')} (should be True)\")\n",
    "print(f\"'2023-12-31': {dt.is_valid_date_format('2023-12-31')} (should be True)\")\n",
    "print(f\"'2023-1-5': {dt.is_valid_date_format('2023-1-5')} (should be False - wrong format)\")\n",
    "print(f\"'01/15/2023': {dt.is_valid_date_format('01/15/2023')} (should be False - wrong format)\")\n",
    "print(f\"'2023/01/15': {dt.is_valid_date_format('2023/01/15')} (should be False - wrong format)\")\n",
    "print(f\"'Jan 15, 2023': {dt.is_valid_date_format('Jan 15, 2023')} (should be False - wrong format)\")\n",
    "print(f\"'2023-13-45': {dt.is_valid_date_format('2023-13-45')} (should be False - invalid date)\")\n",
    "print(f\"None: {dt.is_valid_date_format(None)} (should be False)\")\n",
    "print(f\"empty string: {dt.is_valid_date_format('')} (should be False)\")\n",
    "print(f\"'not a date': {dt.is_valid_date_format('not a date')} (should be False)\")\n",
    "print(f\"'2023-02-29': {dt.is_valid_date_format('2023-02-29')} (should be False - not a leap year)\")\n",
    "print(f\"'2024-02-29': {dt.is_valid_date_format('2024-02-29')} (should be True - leap year)\")\n",
    "print()\n",
    "\n",
    "# Test edge cases\n",
    "print(\"Testing edge cases:\")\n",
    "print(f\"'2023-01-01': {dt.format_date_to_standard('2023-01-01')} (should be 2023-01-01)\")\n",
    "print(f\"'2023-12-31': {dt.format_date_to_standard('2023-12-31')} (should be 2023-12-31)\")\n",
    "print(f\"'1900-01-01': {dt.format_date_to_standard('1900-01-01')} (should be 1900-01-01)\")\n",
    "print(f\"'2100-12-31': {dt.format_date_to_standard('2100-12-31')} (should be 2100-12-31)\")\n",
    "print(f\"'02/29/2024': {dt.format_date_to_standard('02/29/2024')} (should be 2024-02-29 - leap year)\")\n",
    "print(f\"'02/29/2023': {dt.format_date_to_standard('02/29/2023')} (should be None - not leap year)\")\n",
    "print(f\"123456789: {dt.format_date_to_standard(123456789)} (should handle numeric input)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Info:\n",
      "Shape: (500, 30)\n",
      "Columns: ['Account Name', 'account_name', 'AccountName', 'Contact Email', 'contact_email', 'Created Date', 'created_date', 'Lead Source', 'lead_source', 'Opportunity Amount', 'opportunity_amount', 'Is Active', 'is_active', 'SFDC ID', 'sfdc_id', 'Annual Revenue', 'annual_revenue', 'Last Activity', 'Custom Field', 'Region', 'Unnamed: 0', 'Unnamed: 21', 'Random Notes', 'Deal Score', 'Engagement Level', 'Num Calls', 'Time on Page (sec)', 'City', 'State', 'Country']\n",
      "\n",
      "Clean dataset saved as 'CleanSalesforceData.csv'\n",
      "Clean dataset shape: (500, 25)\n",
      "\n",
      "Sample of consolidated data:\n",
      "   account_name     contact_email created_date     lead_source  \\\n",
      "0     Acme Corp  contact@acme.com   2020-01-01      Trade Show   \n",
      "1        Globex   help@globex.com   2022-12-01    Social Media   \n",
      "2        Globex   help@globex.com         None  Email Campaign   \n",
      "3     Acme Corp  contact@acme.com         None    Social Media   \n",
      "4      Umbrella              None   2024-03-26             Web   \n",
      "5        Globex   help@globex.com   2024-04-03            None   \n",
      "6       Soylent              None   2025-07-17        Referral   \n",
      "7       Soylent              None   2023-12-17   Phone Inquiry   \n",
      "8     Acme Corp  contact@acme.com   2022-12-01      Trade Show   \n",
      "9     Acme Corp  contact@acme.com         None  Email Campaign   \n",
      "10     Umbrella              None   2024-06-03      Trade Show   \n",
      "11        Wonka              None   2025-07-08      Trade Show   \n",
      "12       Globex   help@globex.com   2022-12-01            None   \n",
      "13      Soylent              None   2020-01-01      Trade Show   \n",
      "14      Soylent              None   2020-01-01         Partner   \n",
      "15       Globex   help@globex.com   2023-01-26    Social Media   \n",
      "16    Acme Corp  contact@acme.com   2022-12-01        Referral   \n",
      "17       Globex   help@globex.com         None             Web   \n",
      "18      Soylent              None   2020-01-01    Social Media   \n",
      "19       Globex   help@globex.com         None  Email Campaign   \n",
      "20     Umbrella              None   2020-01-01    Social Media   \n",
      "21     Umbrella              None   2024-08-12        Referral   \n",
      "22        Wonka              None   2022-12-01      Trade Show   \n",
      "23       Globex   help@globex.com   2022-12-01  Email Campaign   \n",
      "24      Soylent              None   2020-01-01  Email Campaign   \n",
      "25       Globex   help@globex.com   2024-12-27    Social Media   \n",
      "26    Acme Corp  contact@acme.com   2022-12-01    Social Media   \n",
      "27      Soylent              None   2023-05-04  Email Campaign   \n",
      "28      Initech              None         None             Web   \n",
      "29      Soylent              None         None  Email Campaign   \n",
      "\n",
      "   Opportunity Amount opportunity_amount Is Active is_active     SFDC ID  \\\n",
      "0                1000                NaN       NaN         0      abc123   \n",
      "1                 NaN                NaN     False       NaN         NaN   \n",
      "2               50000                NaN    active      True  001A000001   \n",
      "3                 NaN     Fifty Thousand       NaN         0         NaN   \n",
      "4                 NaN     Fifty Thousand       NaN         0      abc123   \n",
      "5               50000                  0       NaN      True  001A000001   \n",
      "6        Ten Thousand                NaN        no     False  001A000001   \n",
      "7                1000              25000    active      True         NaN   \n",
      "8                1000                  0      True     False   XYZ-00001   \n",
      "9               50000     Fifty Thousand     False     False         NaN   \n",
      "10                NaN              25000    active      True  001A000001   \n",
      "11                NaN     Fifty Thousand    active         1         NaN   \n",
      "12               1000              25000        no         1      abc123   \n",
      "13              50000              25000      True         1         NaN   \n",
      "14                NaN     Fifty Thousand     False         0      abc123   \n",
      "15                NaN     Fifty Thousand     False         1      abc123   \n",
      "16              50000                  0       yes      True      abc123   \n",
      "17                NaN              25000        no      True      abc123   \n",
      "18       Ten Thousand                NaN       NaN       NaN  001A000001   \n",
      "19                NaN                NaN     False     False  001A000001   \n",
      "20               1000                NaN       yes         0      abc123   \n",
      "21       Ten Thousand                NaN      True       NaN   XYZ-00001   \n",
      "22              50000                NaN        no     False         NaN   \n",
      "23              50000                NaN    active         1   XYZ-00001   \n",
      "24                NaN                NaN    active         1         NaN   \n",
      "25                NaN                  0       NaN       NaN   XYZ-00001   \n",
      "26       Ten Thousand                NaN       yes      True   XYZ-00001   \n",
      "27                NaN                NaN      True       NaN      abc123   \n",
      "28              50000                NaN    active     False         NaN   \n",
      "29               1000              25000    active      True  001A000001   \n",
      "\n",
      "       sfdc_id  ... Unnamed: 0  Unnamed: 21 Random Notes Deal Score  \\\n",
      "0   001B000002  ...          0          NaN          NaN       50.0   \n",
      "1       BAD_ID  ...          1          NaN          NaN        NaN   \n",
      "2        12345  ...          2          NaN          NaN        NaN   \n",
      "3       BAD_ID  ...          3          NaN          NaN      100.0   \n",
      "4   001B000002  ...          4          NaN          NaN      100.0   \n",
      "5   001B000002  ...          5          NaN          NaN        NaN   \n",
      "6   001C000003  ...          6          NaN          NaN      100.0   \n",
      "7          NaN  ...          7          NaN    See notes       10.0   \n",
      "8   001B000002  ...          8          NaN          NaN       50.0   \n",
      "9   001C000003  ...          9          NaN    See notes        NaN   \n",
      "10      BAD_ID  ...         10          NaN    See notes       75.0   \n",
      "11      BAD_ID  ...         11          NaN    See notes      100.0   \n",
      "12      BAD_ID  ...         12          NaN          NaN       75.0   \n",
      "13  001B000002  ...         13          NaN        Valid      100.0   \n",
      "14         NaN  ...         14          NaN    See notes      100.0   \n",
      "15       12345  ...         15          NaN          NaN       10.0   \n",
      "16  001C000003  ...         16          NaN    See notes        NaN   \n",
      "17         NaN  ...         17          NaN    See notes      100.0   \n",
      "18      BAD_ID  ...         18          NaN        Valid        NaN   \n",
      "19  001B000002  ...         19          NaN          NaN      100.0   \n",
      "20      BAD_ID  ...         20          NaN          NaN      100.0   \n",
      "21      BAD_ID  ...         21          NaN    See notes       75.0   \n",
      "22  001B000002  ...         22          NaN    See notes       75.0   \n",
      "23  001B000002  ...         23          NaN    See notes       75.0   \n",
      "24      BAD_ID  ...         24          NaN          NaN       10.0   \n",
      "25         NaN  ...         25          NaN    See notes       50.0   \n",
      "26       12345  ...         26          NaN        Valid        NaN   \n",
      "27      BAD_ID  ...         27          NaN          NaN        NaN   \n",
      "28  001C000003  ...         28          NaN        Valid      100.0   \n",
      "29         NaN  ...         29          NaN          NaN      100.0   \n",
      "\n",
      "   Engagement Level  Num Calls  Time on Page (sec)           City       State  \\\n",
      "0               NaN        NaN                 141        Chicago        N.Y.   \n",
      "1          0.063609       31.0                 278            NaN          CA   \n",
      "2          0.217333       38.0                 271        Chicago         NaN   \n",
      "3               NaN       32.0                 318            NaN         NaN   \n",
      "4          0.537578        NaN                 399  San Francisco        N.Y.   \n",
      "5          0.766073       23.0                 383        Chicago    Illinois   \n",
      "6               NaN        3.0                 155            NaN    Illinois   \n",
      "7          0.903417       37.0                 248       New York    Illinois   \n",
      "8          0.541393        NaN                 419       New York          IL   \n",
      "9               NaN       10.0                 108       New York          NY   \n",
      "10         0.030900       49.0                 182  San Francisco    Illinois   \n",
      "11         0.154336       13.0                 194        chicago  California   \n",
      "12              NaN        NaN                 279            NaN    Illinois   \n",
      "13         0.785308       26.0                  37            NaN          NY   \n",
      "14         0.274271       17.0                 350            nyc          NY   \n",
      "15              NaN       21.0                 105        Chicago  California   \n",
      "16         0.899141        NaN                 331        chicago          CA   \n",
      "17         0.060635        9.0                 296            nyc          CA   \n",
      "18              NaN       41.0                 169            NaN  California   \n",
      "19         0.280493       44.0                 353            nyc          CA   \n",
      "20         0.095622        NaN                 360       New York         NaN   \n",
      "21              NaN       41.0                 472            NaN         NaN   \n",
      "22         0.780823       37.0                  73  San Francisco        N.Y.   \n",
      "23         0.290363        1.0                 113       New York          NY   \n",
      "24              NaN        NaN                  21            nyc          IL   \n",
      "25         0.894061        8.0                  50            NaN          IL   \n",
      "26         0.053003       43.0                 205            NaN         NaN   \n",
      "27              NaN       28.0                 107       New York  California   \n",
      "28         0.940728        NaN                 203        chicago        N.Y.   \n",
      "29         0.958253       19.0                 134        chicago        N.Y.   \n",
      "\n",
      "          Country  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2   United States  \n",
      "3   United States  \n",
      "4   United States  \n",
      "5              us  \n",
      "6              us  \n",
      "7             USA  \n",
      "8   United States  \n",
      "9              us  \n",
      "10  United States  \n",
      "11             us  \n",
      "12           U.S.  \n",
      "13            NaN  \n",
      "14            NaN  \n",
      "15             us  \n",
      "16            NaN  \n",
      "17             us  \n",
      "18           U.S.  \n",
      "19           U.S.  \n",
      "20           U.S.  \n",
      "21            USA  \n",
      "22  United States  \n",
      "23             us  \n",
      "24             us  \n",
      "25             us  \n",
      "26            USA  \n",
      "27           U.S.  \n",
      "28             us  \n",
      "29           U.S.  \n",
      "\n",
      "[30 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/DirtySalesforceData.csv')\n",
    "\n",
    "print(\"Original Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "cleaner = Cleansing(df)\n",
    "cleaner.create_clean_dataset()\n",
    "\n",
    "clean_df = cleaner.create_clean_dataset()\n",
    "\n",
    "clean_df.to_csv('CleanSalesforceData.csv', index=False)\n",
    "print(f\"\\nClean dataset saved as 'CleanSalesforceData.csv'\")\n",
    "print(f\"Clean dataset shape: {clean_df.shape}\")\n",
    "\n",
    "print(f\"\\nSample of consolidated data:\")\n",
    "print(clean_df.head(30))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
